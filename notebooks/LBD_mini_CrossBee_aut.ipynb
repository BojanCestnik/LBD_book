{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossBee workflow notebook tutorial based on Python script modules for LBD\n",
    "\n",
    "CrossBee is a system that recommends bridging terms (*b-terms*) through an ensemble-based ranking method. It aids experts in uncovering hidden connections between unrelated domains. CrossBee facilitates the ranking, exploration, and investigation of these cross-domain bridging terms.\n",
    "\n",
    "<hr>\n",
    "\n",
    "[2] Juršič, M., Cestnik, B., Urbančič, T., Lavrač, N. (2012a). Cross-domain Literature Mining: Finding Bridging Concepts with CrossBee, *Proceedings of the 3rd International Conference on Computational Creativity*, 33-40.\n",
    "\n",
    "[3] Juršič, M., Cestnik, B., Urbančič, T., Lavrač, N. (2012b). Bisociative literature mining by ensemble heuristics. In M. R. Berthold (Ed.), *Bisociative Knowledge Discovery*, 338–358. Springer.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Note that **our motive** was to **re-implement parts** of the tools such as **CrossBee**, RaJoLink and OntoGen so that we can generally **repeat the results** with the tools from the past experiments, and not so much to optimize the written Python code. The emphasis was on the reader's understanding of the learning processes and visualizing the workflows in terms of the repeatability of the results obtained; the efficiency and elegance of the programming was often saved for later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize `logging` library to track the execution of the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement: \n",
    "* Determine outlier documents (OntoGen)\n",
    "* Draw ROC curve (CrossBee)\n",
    "* Ensemble heuristics (CrossBee)\n",
    "\n",
    "TODO:\n",
    "* CrossBee (ROC in Ensemble, potential outliers)\n",
    "* OntoGen (potential outliers)\n",
    "* Domains:\n",
    "    - Autism - Calcineurin (RaJoLink, CrossBee)\n",
    "    - Alzheimer - Macrobiota (OntoGen, CrossBee)\n",
    "    - Migraine - Magnesium (RaJoLink - Migraine, CrossBee, OntoGen)\n",
    "    - Plant defense - Circadian rythm (???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from the framework notebooks. The description of the individual components from the framework notebooks can be found in the respective notebooks. \n",
    "\n",
    "The purpose of the **import_ipynb** library is to enable the direct import of Jupyter notebooks as modules so that code, functions, and classes defined in one notebook can be easily reused in other notebooks or Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name of the domains $C$ and $A$, then load the responding text from the file. The expected file format is as follows:\n",
    "\n",
    "1. The file is encoded in Ascii (if it is in UTF-8 or other encoding, it should be converted to Ascii).\n",
    "2. Each line in the file represents one document. The words in each document are separated by spaces. The length of the individual documents may vary.\n",
    "3. The first word in each line is the **unique id**, followed by a semicolon. Normally **pmid** (pubmed id) can be used for this purpose. Or, as in this case of **autism-calcineurin** domains, simple count was used, due to historical reasons, to label the documents.\n",
    "4. The second word in each line can optionally stand for a predefined domain (or class) of the document. In this case, the second word is preceded by **!**. For example, if the file contains documents that originate from two domains, e.g. *autism* and *calcineurin*, the second word in each line is either **!autism** or **!calcineurin**.\n",
    "5. If the second word is not preceded by **!**, it will be considered the first word of the document. In this case, the document will be given the domain **!NA** (**not applicable** or **not available**).\n",
    "\n",
    "**A background story for this experiment**\n",
    "\n",
    "First, we selected *autism* and \"calcineurin\" as our domains of interest. \n",
    "\n",
    "Then we searched PubMed and collected 214 full-text documents on autism from the decade before 2006 in PubMed Central. After collecting the documents, we converted them from HTML and PDF format to plain text and made sure that each document was formatted consistently for further analysis. The 214 full text documents are stored in the file `input/Autism_Calcineurin.txt`.\n",
    "\n",
    "We extracted around 2000 unique terms, focusing particularly on rare terms from the fields of amino acids, peptides and proteins to assess their potential relevance to autism research. Notable rare terms such as *lactoylglutathione*, *synaptophysin* and *calcium channels* appeared in our dataset.\n",
    "\n",
    "The selected rare terms *lactoylglutathione*, *synaptophysin* and *calcium channels* prompted our team's autism expert to specifically investigate their associations with *calcineurin* (as it appeared as a joint term in all literatures of the selected rare terms). *Calcineurin* is a protein phosphatase with a high prevalence in the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#domainName = 'Migraine-Magnesium'\n",
    "#fileName = 'input/Magnesium_Migraine_before1988.txt'\n",
    "#lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "#lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domainName = 'Autism-Calcineurin'\n",
    "fileName = 'input/Autism_Calcineurin.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainName = 'Demo bow'\n",
    "fileName = 'input/demo_bow.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines[9362:9369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess documents into a dictionary and extract documents as strings in a list. Might take a few minutes for longer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = ['autism', 'calcineurin']\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 4, keep_only_nouns = True, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = False, lemmatization = True, \\\n",
    "    min_word_length = 1, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first dictionary items, document (pubmed) ids and preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prep_docs_list[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate bag-of-words matrix from the list of preprocessed documents. Remove ngram words that occur less than *min_ngram_count* times (3 in our case) in the whole corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 1 # 3 for migraine\n",
    "min_df = 1\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all ngrams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(bow_matrix)\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3 # 2 for migraine\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        # if word in mesh_word_list:\n",
    "        tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            tmp_filter_columns.append(i)\n",
    "\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent ngrams: ', len(word_list))\n",
    "\n",
    "LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for bag-of-word matrix. Prepare also the dictionaries of words and documents sorted according to the sum of word count in BOW matrix (*bow_matrix*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize bag-of-words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tf-idf matrix from the list of preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all ngrams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "# Rearange (filter) the tfidf matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent ngrams: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print('Sum of tf-idf for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of tf-idf for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of tf-idf for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of tf-idf for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the tfidf matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of the domain names of all the documents and a list of unique domain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate domains_bow_matrix from bow_matrix using domain_names list to add bow_matrix rows for each unique domain name into a single row\n",
    "domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "no_documents_in_domain = {}\n",
    "for i, domain_name in enumerate(unique_domains_list):\n",
    "    domain_docs_indices = [i for i, label in enumerate(domains_list) if label == domain_name]\n",
    "    no_documents_in_domain[domain_name] = len(domain_docs_indices)\n",
    "    print(domain_docs_indices[:7])\n",
    "    tmp = (bow_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    print(i, tmp)\n",
    "    domains_bow_matrix = np.vstack((domains_bow_matrix, tmp))\n",
    "    # Compute centroid for the current cluster\n",
    "    #centroid_x = np.mean(pca_result[cluster_docs_indices, 0])\n",
    "    #centroid_y = np.mean(pca_result[cluster_docs_indices, 1])\n",
    "print(domains_bow_matrix)\n",
    "print(no_documents_in_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_value_in_bow_matrix(bow_matrix, domain_name, word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_idx = unique_domains_list.index(domain_name)\n",
    "    column_idx = word_list.index(word)\n",
    "    return(bow_matrix[line_idx, column_idx])\n",
    "\n",
    "cell_value_in_bow_matrix(domains_bow_matrix, unique_domains_list[0], word_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Seed the random number generator for better randomness\n",
    "random.seed(time.time())  # Use current time to seed the generator\n",
    "\n",
    "# Generate a random 7-digit number\n",
    "random_number = random.randint(1000000, 9999999)\n",
    "\n",
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "sum_word_tfidf_selected = {} \n",
    "aux_word_tfidf_selected = {} \n",
    "min_word_tfidf_selected = {} ### TODO from here on ...\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] >= 1:\n",
    "        passed = True\n",
    "        for domain_name in unique_domains_list:\n",
    "            if cell_value_in_bow_matrix(domains_bow_matrix, domain_name, word) <= 0:\n",
    "                passed = False\n",
    "        if passed:\n",
    "            max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "            sum_word_tfidf_selected[word] = sum_word_tfidf[word]\n",
    "            aux_word_tfidf_selected[word] = random.randint(1, 999999)\n",
    "            min_word_tfidf_selected[word] = max_word_tfidf[word] * sum_word_tfidf[word]\n",
    "\n",
    "max_word_tfidf_selected = aux_word_tfidf_selected\n",
    "# max_word_tfidf_selected = sum_word_tfidf_selected\n",
    "# max_word_tfidf_selected = min_word_tfidf_selected\n",
    "\n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected b-term candidate words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted b-term candidate words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('The first and the last sorted b-term word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max tf-idf values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bterms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "bterms_list_length = len(bterms_list)\n",
    "\n",
    "df = pd.DataFrame({'b-term': bterms_list, 'max tf-idf': list(max_word_tfidf_selected_sorted.values())})\n",
    "df[0:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'bcl2'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petric_bterms = [\"22q112\", \"deletion syndrome\", \"asbestos\", \"bcl2\", \"bombesin\", \"calmodulin\", \"radiation\", \\\n",
    "                 \"hypothyroxinemia\", \"synaptic\", \"synaptic plasticity\", \"type 1 diabetes\", \\\n",
    "                 \"ulcerative colitis\", \"working memory\", \\\n",
    "                 \"t17p22q21\"]\n",
    "\n",
    "petric_bterms = [\"22q112\", \"deletion\", \"syndrome\", \"asbestos\", \"bcl2\", \"bombesin\", \"calmodulin\", \"radiation\", \\\n",
    "                 \"hypothyroxinemia\", \"synaptic\", \"plasticity\", \"diabetes\", \\\n",
    "                 \"colitis\", \"work\", \"memory\", \"t17p22q21\"]\n",
    "nn = 0\n",
    "indb = []\n",
    "size = len(max_word_tfidf_selected_sorted)\n",
    "for name in petric_bterms:\n",
    "    if name in max_word_tfidf_selected_sorted.keys():\n",
    "       nn += 1\n",
    "       position = list(max_word_tfidf_selected_sorted.keys()).index(name)\n",
    "       indb.append(position)\n",
    "       print(name, ': ', 'position in the list of potential bterms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "             '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), ' part: ', format(position/size*100, '.1f'), sep='')\n",
    "    else:\n",
    "        print('NOT:', name, 'NOT in the list.')   \n",
    "print(nn, len(petric_bterms))\n",
    "print(indb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing and changed b-terms:\n",
    "\n",
    "- maternal hypothyroxinemia -> hypothyroxinemia\n",
    "- ulcerative colitis -> colitis\n",
    "- t17p22q21 - found only in Autism documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement selected heuristics for bterm ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = []\n",
    "for i in range(size):\n",
    "    pt.append(0)\n",
    "for i in range(len(indb)):\n",
    "    for j in range(indb[i], size):\n",
    "        pt[j] += 1\n",
    "print(pt)\n",
    "suma = 0\n",
    "part = 0\n",
    "for i in range(size):\n",
    "    print((i+1)/size*100.0, pt[i]/len(indb)*100.0)\n",
    "    part += pt[i]\n",
    "    suma += len(indb)\n",
    "print(part/suma*100.0)\n",
    "\n",
    "no_all_bterm_candidates = size\n",
    "no_swansons_bterms = len(indb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example ROC curve points (y-values), replace with the actual list of 120 numbers\n",
    "roc_points = pt\n",
    "\n",
    "# X-values for the ROC curve (0 to 120)\n",
    "x_values = np.arange(0, no_all_bterm_candidates)\n",
    "\n",
    "# Calculating the AUC for the given ROC curve using the trapezoidal rule\n",
    "auc = np.trapz(roc_points, x_values) / (no_all_bterm_candidates*no_swansons_bterms) * 100  # Normalizing by the area of the full plot\n",
    "\n",
    "# Plotting the default curve (50% AUC)\n",
    "default_x = np.array([0, no_all_bterm_candidates])\n",
    "default_y = np.array([0, no_swansons_bterms])\n",
    "plt.plot(default_x, default_y, label='Default Curve (50% AUC)', linestyle='--', color='gray')\n",
    "\n",
    "# Plotting the given ROC curve\n",
    "plt.plot(x_values, roc_points, label=f'Given ROC Curve (AUC: {auc:.2f})', color='blue')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate (Discrete)')\n",
    "plt.ylabel('True Positive Rate (Discrete)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
