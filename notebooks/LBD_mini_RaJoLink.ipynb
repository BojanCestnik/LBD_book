{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RaJoLink workflow notebook tutorial based on Python script modules for LBD\n",
    "\n",
    "RaJoLink is a method developed for **open** literature-based discovery (LBD). In contrast to Swanson's `ABC` model, RaJoLink focuses on a semi-automatic identification of candidates ($a$) that might be related to an investigated phenomenon ($c$). This selection is based on the identification of **rare terms** from the literature on $c$. At the heart of the RaJoLink strategy is the rational assumption that if literatures on several rare terms have a term in common, this term is a candidate for the term $a$.\n",
    "\n",
    "The **RaJoLink** method comprises three main steps: **Ra**, **Jo**, and **Link**, which focus on rare terms, joint terms and linking terms, respectively. The Ra step searches the literature on phenomenon $C$ for unique or rare terms. The Jo step reviews articles related to these rare terms, and identifies joint terms (candidates for $a$) that appear in them, suggesting the hypothesis that $C$ is related to $A$. The Link step then looks for $b$-terms that bridge the literature on a selected $a$-term and $c$-term; $b$-terms are the candidates that can possibly explain the link.\n",
    "\n",
    "The identification of rare terms in the Ra step is based on the statistical principle of outliers. Just as outliers in data can lead to significant discoveries, rare terms in the literature can pave the way for innovative connections. A term is considered rare if it occurs in $n$ or fewer data sets, where $n$ is adjustable depending on the experiment or context.\n",
    "\n",
    "While Swanson's ABC model connects two disjoint literatures with the term $b$, RaJoLink uses rare terms to find the term $a$, which bridges the literatures with selected rare terms.\n",
    "\n",
    "In this particular implementation, the search for b-terms is limited to the expert-selected [MeSH](https://www.nlm.nih.gov/mesh/meshhome.html) words for Enzymes and Coenzymes [D08] and Amino Acids, Peptides, and Proteins [D12]. The main purpose of MeSH filtering is to reduce the vocabulary size, which in turn improves the time complexity of the LBD algorithms used. Considering only the words from the two MeSH categories also reduces the effort for the human expert in guiding and evaluating the results.\n",
    "\n",
    "<hr>\n",
    "\n",
    "[1] Petrič, I., Urbančič, T., Cestnik, B., & Macedoni-Lukšič, M. (2009). Literature mining method RaLoLink for uncovering relations between biomedical concepts. Journal of Biomedical Informatics, 42(2), 219–227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize `logging` library to track the execution of the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from the framework notebooks. The description of the individual components from the framework notebooks can be found in the respective notebooks. \n",
    "\n",
    "The purpose of the **import_ipynb** library is to enable the direct import of Jupyter notebooks as modules so that code, functions, and classes defined in one notebook can be easily reused in other notebooks or Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import json\n",
    "import spacy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the script for the first time, `en_core_web_md`, a medium sized English model trained on written web text (blogs, news, comments), must be downloaded with the command:\n",
    "\n",
    "```python\n",
    "!python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "For the first run, you must therefore comment out the first line in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md \n",
    "\n",
    "nlpr = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Ra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name of the domain $C$ and load the responding text from the file. The expected file format is as follows:\n",
    "\n",
    "1. The file is encoded in Ascii (if it is in UTF-8 or other encoding, it should be converted to Ascii).\n",
    "2. Each line in the file represents one document. The words in each document are separated by spaces. The length of the individual documents may vary.\n",
    "3. The first word in each line is the **unique id**, followed by a semicolon. Normally **pmid** (pubmed id) can be used for this purpose.\n",
    "4. The second word in each line can optionally stand for a predefined domain (or class) of the document. In this case, the second word is preceded by **!**. For example, if the file contains documents that originate from two domains, e.g. *migraine* and *magnesium*, the second word in each line is either **!migraine** or **!magnesium**. If the file contains documents that originate from *autism* and *calcineurin*, the second word in each line will be either **!autism** or **!calcineurin**.\n",
    "5. If the second word is not preceded by **!**, it will be considered the first word of the document. In this case, the document will be given the domain **!NA** (**not applicable** or **not available**).\n",
    "\n",
    "**A background story for this experiment**\n",
    "\n",
    "First, we selected *Autism* as our domain of interest. Then we searched PubMed and collected 214 full-text documents on autism from the decade before 2006 in PubMed Central. After collecting the documents, we converted them from HTML and PDF format to plain text and made sure that each document was formatted consistently for further analysis. The 214 full text documents are stored in the file `input/214Texts.txt`.\n",
    "\n",
    "We extracted around 2000 unique terms, focusing particularly on rare terms from the fields of amino acids, peptides and proteins to assess their potential relevance to autism research. Notable rare terms such as *lactoylglutathione*, *synaptophysin* and *calcium channels* appeared in our dataset.\n",
    "\n",
    "The selected rare terms *lactoylglutathione*, *synaptophysin* and *calcium channels* prompted our team's autism expert to specifically investigate their associations with *calcineurin* (as it appeared as a joint term in all literatures of the selected rare terms). *Calcineurin* is a protein phosphatase with a high prevalence in the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainName = 'Autism'\n",
    "fileName = 'input/214Texts.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "# display the first 7 lines of the document\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script is part of a pipeline that is used to pre-process medical literature data and focuses on terms related to specific MeSH (Medical Subject Headings) categories. In this specific case, it is about loading and preprocessing MESH terms for Enzymes and Coenzymes [D08] and Amino Acids, Peptides, and Proteins [D12]. The input file MESH_D08_D12.txt was created by selecting the relevant [D08] and [D12] terms from the xml file `desc2024.xml`, which was downloaded from <a href=\"https://www.nlm.nih.gov/databases/download/mesh.html\">the MeSH website</a>. The input file contains 3534 words after preprocessing, which are used as filters in the further preprocessing of autism-related files.\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Load data*:\n",
    " The script starts loading MeSH data from a specified file:\n",
    " ```python\n",
    " mesh_lines = LBD_01_data_acquisition.load_data_from_file(\"input/MESH_D08_D12.txt\")\n",
    " ```\n",
    " The file `MESH_D08_D12.txt` contains words that refer to certain MeSH categories (D08 and D12).\n",
    "\n",
    "2. *Dictionary construction*:\n",
    " In the next step, a dictionary is constructed from the loaded lines:\n",
    " ```python\n",
    " mesh_docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(mesh_lines)\n",
    " ```\n",
    " This converts the list of lines into a dictionary (`mesh_docs_dict`) in which the keys represent document identifiers and the values contain the corresponding text. This structure is more efficient for subsequent text processing tasks.\n",
    "\n",
    "3. *Pre-processing of documents*:\n",
    " In the pre-processing phase, the text data is cleaned up and standardized:\n",
    " ```python\n",
    " mesh_prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    " mesh_docs_dict, keep_list=[], remove_list=[], mesh_word_list=[], \\\n",
    " cleaning=True, remove_stopwords=True, lemmatization=True, min_word_length=5)\n",
    " ```\n",
    " Various pre-processing methods are used here:\n",
    " - *Cleaning*: General cleaning of the text.\n",
    " - *Stopword Removal*: Removal of frequent words that do not provide meaningful information (e.g. \"the\", \"and\").\n",
    " - *Lemmatization*: Reduction of words to their basic or root form (e.g. \"running\" becomes \"run\").\n",
    " - *Minimum Word Length*: Filtering out words with less than five characters.\n",
    " These steps prepare the text data for further analysis.\n",
    "\n",
    "4. *Extract pre-processed documents*:\n",
    " After preprocessing, the script extracts the cleaned text back into a list:\n",
    " ```python\n",
    " mesh_prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(mesh_prep_docs_dict)\n",
    " ```\n",
    " This conversion is necessary for feature extraction, where the text must be in a list format.\n",
    "\n",
    "5. *Feature extraction (Bag of Words)*:\n",
    " The last part of the script creates a Bag of Words (BoW) model:\n",
    " ```python\n",
    " mesh_word_list, mesh_bow_matrix = LBD_03_feature_extraction.create_bag_of_words(mesh_prep_docs_list, 1, 1)\n",
    " ```\n",
    " The BoW model is a text representation technique in which:\n",
    " - *`mesh_word_list`* contains the unique words identified in the documents.\n",
    " - *`mesh_bow_matrix`* is a matrix in which each row corresponds to a document and each column represents a word, with the matrix values indicating the frequency of words in the documents.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Biomedical research*: Researchers can use this script to pre-process and analyze large datasets of medical literature to identify new links between diseases, drugs and biological processes.\n",
    "- *Text mining and NLP*: The script can be customized for more comprehensive text mining tasks, such as sentiment analysis, topic modeling or other areas that require structured text representation.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script in your workflow, you need to make sure you have the appropriate data file (`MESH_D08_D12.txt`) and the modules for data collection, preprocessing and feature extraction. After running the script, you will receive a vocabulary list and a corresponding BoW matrix that you can analyze further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_lines = LBD_01_data_acquisition.load_data_from_file(\"input/MESH_D08_D12.txt\")\n",
    "\n",
    "mesh_docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(mesh_lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = []\n",
    "mesh_prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    mesh_docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, min_word_length = 5)\n",
    "\n",
    "mesh_prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(mesh_prep_docs_dict)\n",
    "\n",
    "mesh_word_list, mesh_bow_matrix = LBD_03_feature_extraction.create_bag_of_words(mesh_prep_docs_list, 1, 1)\n",
    "print('Number of terms in MESH D08 and D12 vocabulary: ', len(mesh_word_list))\n",
    "print('First 7 words in the mesh_word_list:', mesh_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script in the next cell is used to prepare text data for further analysis in Literature-Based Discovery (LBD). The aim is to clean, standardize and structure the documents so that they are suitable for further tasks such as feature extraction, topic modeling and the discovery of hidden relationships in the literature. The script prepares the documents stored in `lines` in a dictionary and then processes the documents with the obtained MeSH word list of Enzymes and Coenzymes [D08] and Amino Acids, Peptides and Proteins [D12].\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Creating a dictionary from raw data*:\n",
    " The script starts by converting a list of rows into a structured dictionary:\n",
    " ```python\n",
    " docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    " ```\n",
    " - *`construct_dict_from_list`**: This function takes the raw list of text lines (`lines`) and creates a dictionary (`docs_dict`) in which each entry typically represents a document, with a unique identifier as the key and the text of the document as the value.\n",
    " - This conversion is important because it puts the text data into a more manageable format that allows efficient processing and retrieval.\n",
    "\n",
    "2. *Preprocessing of documents*:\n",
    " The script then applies various pre-processing steps to the documents:\n",
    " ```python\n",
    " keep_list = []\n",
    " remove_list = []\n",
    " prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    " docs_dict, keep_list=keep_list, remove_list=remove_list, mesh_word_list=mesh_word_list, \\\n",
    " cleaning=True, remove_stopwords=True, lemmatization=True, \\\n",
    " min_word_length=5, keep_only_nouns=False, keep_only_mesh=True, stemming=False, stem_type=None)\n",
    " ```\n",
    " - *Cleaning*: The text is cleaned to remove unwanted characters, punctuation and other errors.\n",
    " - *Remove stop words*: Frequent words that do not provide meaningful information (e.g. \"the\", \"and\") are removed.\n",
    " - *Lemmatization*: Words are reduced to their base or root form (e.g. \"running\" becomes \"run\") to ensure consistency.\n",
    " - *Minimum word length*: Words shorter than five characters are filtered out.\n",
    " - *MeSH-specific filtering*: The parameter `keep_only_mesh=True` ensures that only terms from the Medical Subject Headings (MeSH) vocabulary are considered in order to focus the analysis on relevant biomedical terminology.\n",
    "\n",
    "This pre-processing step is important to reduce noise and focus on the most important terms, which improves the quality of subsequent analyses.\n",
    "\n",
    "3. *Extract document IDs and processed text*:\n",
    " The script then extracts lists of document IDs and the corresponding preprocessed text:\n",
    " ```python\n",
    " ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    " prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)\n",
    " ```\n",
    " - *`extract_ids_list`*: Returns a list of document IDs from the preprocessed dictionary to facilitate document lookup and management.\n",
    " - *`extract_preprocessed_documents_list`*: Extracts the cleaned and processed text for each document to prepare it for feature extraction or other analysis.\n",
    "\n",
    " By extracting these lists, the script organizes the data in a format that is easy to manipulate in subsequent steps, such as creating a Bag of Words (BoW) model or calculating TF-IDF scores.\n",
    "\n",
    "**Applications**\n",
    "\n",
    "- *Biomedical text mining**: This pre-processing approach is valuable in the biomedical field, where ensuring the relevance and accuracy of terms is critical to discovering new relationships between diseases, drugs and other biological concepts.\n",
    "- *Data preparation for machine learning*: The cleaned and structured data generated by this script can be fed directly into machine learning models for tasks such as document classification or clustering.\n",
    "- *Research and hypothesis generation*: By focusing on specific vocabularies such as MeSH, researchers can more effectively search the literature for new hypotheses or overlooked relationships.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script effectively:\n",
    "1. *Prepare the data*: Make sure you have a list of raw text lines (`lines`) and a corresponding vocabulary list (e.g. `mesh_word_list`).\n",
    "2. *Execute the preprocessing steps*: Run the script to clean, filter and structure the text data.\n",
    "3. *Extract and analyze*: Use the extracted IDs and processed text for further analysis, e.g. to create models and visualizations or for exploratory research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = []\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = mesh_word_list, \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 5, keep_only_nouns = False, keep_only_mesh = True, stemming = False, stem_type = None)\n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells show the first dictionary entries, the document IDs (Pubmed) and the pre-processed documents.\n",
    "\n",
    "When displaying the dictionary entries, we can see the difference between the original and the pre-processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 7 dictionary items\n",
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the ids of the first 7 documents\n",
    "ids_list[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the preprocessed text for the first 7 documents\n",
    "prep_docs_list[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script continues the feature extraction process and focuses on refining a Bag of Words (BoW) model by filtering out less important terms and n-grams. It creates a Bag of Words matrix from the list of pre-processed documents. It then removes n-gram words that occur less than *min_ngram_count* times (in our case 3) in the entire document corpus. The words that are not contained in the MESH list *mesh_word_list* are also removed. This step is important to improve the quality and relevance of the text representation by reducing the vocabulary so that the following steps can be carried out more efficiently (in terms of time).\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Set parameters*:\n",
    "    The script starts by setting the parameters for the n-gram size and the minimum document frequency:\n",
    "    ```python\n",
    "    ngram_size = 2\n",
    "    min_df = 1\n",
    "    ```\n",
    "    - *`ngram_size`*: Specifies that the model considers pairs of consecutive words (bigrams) as features.\n",
    "    - *`min_df`*: Specifies the minimum number of documents in which a word or n-gram must occur in order to be included in the initial vocabulary.\n",
    "\n",
    "2. *Create Bag of Words representation*:\n",
    "    The next step is to create the BoW model using the specified n-gram size:\n",
    "    ```python\n",
    "    word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "    print('Number of terms in the source vocabulary with all n-grams: ', len(word_list))\n",
    "    ```\n",
    "    This function creates a vocabulary (`word_list`) from all terms and n-grams found in the preprocessed documents (`prep_docs_list`), together with the corresponding frequency matrix (`bow_matrix`). The output vocabulary includes all n-grams without filtering.\n",
    "\n",
    "3. *Filtering low-frequency n-grams*:\n",
    "    The script then filters out n-grams that occur less frequently than a certain threshold:\n",
    "    ```python\n",
    "    min_count_ngram = 3\n",
    "    tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "    tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "    ```\n",
    "    - *`min_count_ngram`*: Specifies the minimum number of occurrences of n-grams to keep.\n",
    "    - *`keep_only_mesh_II`*: If true, only the n-grams with each word in MeSH are kept.\n",
    "    - The script calculates two important metrics:\n",
    "        - *document frequency*: How many documents contain each word or n-gram.\n",
    "        - *Total frequency*: How often each word or n-gram appears in all documents.\n",
    "\n",
    "4. *Filtering Based on Specific Criteria*:  \n",
    "   The script applies a more sophisticated filtering process to refine the vocabulary:\n",
    "   ```python\n",
    "   tmp_filter_columns = []\n",
    "   for i, word in enumerate(word_list):\n",
    "       if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "           if (not keep_only_mesh_II) or (word in mesh_word_list):\n",
    "               tmp_filter_columns.append(i)\n",
    "       else:\n",
    "           if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "               check_ngram = word.split()\n",
    "               passed = True\n",
    "               for check_word in check_ngram:\n",
    "                   if keep_only_mesh_II:\n",
    "                       if check_word not in mesh_word_list:\n",
    "                           passed = False\n",
    "               if check_ngram [0] == check_ngram [1]:\n",
    "                   passed = False\n",
    "               if passed:\n",
    "                   tmp_filter_columns.append(i)\n",
    "   ```\n",
    "   This loop evaluates each term or n-gram in the vocabulary:\n",
    "   - *Non-n-grams*: Will only be retained if they are in a predefined `mesh_word_list`.\n",
    "   - *n-grams*: Are retained if:\n",
    "       - They fulfill the minimum frequency criteria.\n",
    "       - All partial words are contained in `mesh_word_list`.\n",
    "       - The n-gram does not consist of repeated words (e.g. \"word word\").\n",
    "\n",
    "\n",
    "5. *Applying the filters*:  \n",
    "   The script then filters both the rows and the columns of the BoW matrix:\n",
    "   ```python\n",
    "   tmp_filter_rows = []\n",
    "   for i, id in enumerate(ids_list):\n",
    "       tmp_filter_rows.append(i)\n",
    "\n",
    "   tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "       word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "   word_list = tmp_filtered_word_list\n",
    "   bow_matrix = tmp_filtered_bow_matrix\n",
    "   print('Number of terms in the preprocessed vocabulary after removing infrequent n-grams and non-MESH words: ', len(word_list))\n",
    "   ```\n",
    "   - *`filter_matrix_columns`*: Refines the BoW matrix by retaining only the selected words or n-grams that meet the filter criteria.\n",
    "   - The updated vocabulary and matrix are then stored in `word_list` and `bow_matrix`, respectively.\n",
    "\n",
    "**Applications**\n",
    "\n",
    "- *Medical text mining*: This filtering method is particularly useful in medical research, where the focus is on extracting and analyzing relevant biomedical terms and concepts.\n",
    "- *Document classification*: By refining the feature set, this script can improve the performance of classifiers used in the categorization of scientific literature or other text corpora.\n",
    "- *Network analysis*: The filtered vocabulary can serve as a node in a network graph representing meaningful terms and their co-occurrence, which can be analyzed to detect hidden connections.\n",
    "\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script effectively, you need to make sure you have a preprocessed document list (`prep_docs_list`) and a list of MeSH terms (`mesh_word_list`). Adjust the parameters like `ngram_size`, `min_df` and `min_count_ngram` to your specific needs. After running the script, you will get a filtered vocabulary and a corresponding BoW matrix, which is more suitable for further analysis such as clustering, topic modeling or discovering new hypotheses in biomedical research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 2\n",
    "min_df = 1\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3\n",
    "keep_only_mesh_II = True\n",
    "\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        if (not keep_only_mesh_II) or (word in mesh_word_list):\n",
    "            tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            check_ngram = word.split()\n",
    "            passed = True\n",
    "            for check_word in check_ngram:\n",
    "                if keep_only_mesh_II:\n",
    "                    if check_word not in mesh_word_list:\n",
    "                        passed = False\n",
    "            if check_ngram[0] == check_ngram[1]:\n",
    "                passed = False\n",
    "            if passed:\n",
    "                tmp_filter_columns.append(i)\n",
    "\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "\n",
    "kom_text = ''\n",
    "if keep_only_mesh_II:\n",
    "    kom_text = ' and non-MeSH words'\n",
    "print('Number of terms in the preprocessed vocabulary after removing infrequent n-grams', kom_text, ': ', len(word_list), sep='')\n",
    "\n",
    "LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script in the next cell is a continuation of the text preprocessing pipeline that calculates the margins for the Bag of Words (BoW) matrix and optimizes the BoW matrix for better interpretability and analysis. By arranging the matrix to highlight the most important terms and documents, this script helps to recognize patterns in the data, which is a crucial step in LBD.\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Counting word frequencies*:\n",
    "   The script begins by calculating various frequency counts that provide insight into how words are distributed across documents:\n",
    "   ```python\n",
    "   sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "   sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "   sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "   ```\n",
    "   - *`sum_count_docs_containing_word`*: Counts how many documents each word appears in.\n",
    "   - *`sum_count_word_in_docs`*: Totals the occurrences of each word across all documents.\n",
    "   - *`sum_count_words_in_doc`*: Tallies the total number of words in each document.\n",
    "\n",
    "   These metrics are essential for understanding the significance and distribution of terms within the corpus, which can guide further analysis.\n",
    "\n",
    "2. *Displaying frequency counts*:\n",
    "   The script then prints a subset of these frequency counts to give an overview of the data:\n",
    "   ```python\n",
    "   print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "   print('Number of occurrences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "   print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "   ```\n",
    "   - *`islice`* from `itertools` is used to print just the first few items, making it easier to inspect the data without overwhelming output.\n",
    "   - These print statements help users quickly assess the distribution and frequency of words and documents in the BoW model.\n",
    "\n",
    "3. *Optimizing the BoW matrix*:\n",
    "   The script proceeds to rearrange the BoW matrix so that the most frequent words and documents are positioned at the top-left corner of the matrix:\n",
    "   ```python\n",
    "   filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "       LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "   filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "       LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list)\n",
    "   ```\n",
    "   - *sorting*: The words and documents are sorted by their frequencies in descending order.\n",
    "   - *filtering*: The indices of these sorted words and documents are then used to rearrange the BoW matrix.\n",
    "\n",
    "   This step ensures that the most significant terms and documents are easily accessible, facilitating further analysis such as clustering, topic modeling, or visualization.\n",
    "\n",
    "4. *Rearranging the matrix*:\n",
    "   Finally, the script filters the matrix according to the computed order:\n",
    "   ```python\n",
    "   filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "       ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "   ```\n",
    "   - *`filter_matrix`*: This function reorders the BoW matrix based on the sorted indices, ensuring that the most relevant terms and documents are emphasized.\n",
    "\n",
    "   The script then prints out the first few items in the reordered lists:\n",
    "   ```python\n",
    "   print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "   print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])\n",
    "   ```\n",
    "   - This output allows users to verify that the matrix has been rearranged as intended, highlighting the most important elements of the dataset.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script, you must have a BoW matrix (`bow_matrix`) and the corresponding lists of words (`word_list`) and document IDs (`ids_list`). The script processes these inputs to calculate the frequency counts, reorder the matrix and output the reordered BoW matrix. This optimized matrix can be used for various downstream tasks, e.g. for creating visualizations, for deeper statistical analysis or as a basis for machine learning models for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize left upper part of the Bag of Words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script is designed to create a Term Frequency-Inverse Document Frequency (TF-IDF) matrix from a set of preprocessed documents and then refine this matrix by filtering out less relevant terms.\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Creating the TF-IDF matrix*:<br>\n",
    "   The script begins by generating a TF-IDF matrix using a list of preprocessed documents:\n",
    "   ```python\n",
    "   word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "   ```\n",
    "   - *TF-IDF matrix*: This matrix represents the importance of each word (or n-gram) across all documents in the corpus.\n",
    "   - *`ngram_size`*: Specifies the size of word sequences to consider (e.g., unigrams, bigrams).\n",
    "   - *`min_df`*: Filters out terms that appear in fewer than a specified number of documents, reducing noise in the analysis.\n",
    "\n",
    "   This step is essential for transforming raw text data into a structured format that highlights important terms.\n",
    "\n",
    "2. *Rearranging the TF-IDF matrix*:\n",
    "   The script then refines the TF-IDF matrix by rearranging and filtering the terms:\n",
    "   ```python\n",
    "   tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "       word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "   word_list = tmp_filtered_word_list\n",
    "   tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "   ```\n",
    "   - *filtering*: The matrix is filtered based on criteria such as the importance of terms, ensuring that only the most relevant words remain.\n",
    "   - *rearranging*: The matrix is reorganized according to a predefined order, based on the significance of terms or their relevance to specific documents.\n",
    "\n",
    "   This refinement process is crucial for improving the quality of the analysis by focusing on the most impactful terms, which can lead to more accurate and insightful results.\n",
    "\n",
    "**Use**\n",
    "\n",
    "Users can apply this script as part of a larger text mining workflow where the TF-IDF matrix serves as an important step in structuring and analyzing the data. By filtering and refining the matrix, users can ensure that their analysis focuses on the most relevant and meaningful terms, leading to more meaningful insights. In the context of LBD, this script is an essential tool for turning raw text data into actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# Rearange (filter) the tfidf matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams and non MESH words: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print('Sum of tfidf for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of tfidf for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the tfidf matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the left upper part of the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TF-IDF', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of domain names of all documents (from the dictionary containing the documents) and a list of unique domain names. Since all the documents are from the autism domain and there is no specific domain name for each document, we expect the only domain to be *NA*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the documents in a 2D graph by reducing the dimensionality of the TF-IDF matrix with PCA. Visualizing TF-IDF data with PCA helps to understand complex, high-dimensional data by projecting it into a more interpretable form. This is crucial in LBD, as understanding the relationships between documents can lead to the discovery of new knowledge or the identification of new connections between concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, domains_list, tfidf_matrix, transpose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the TF-IDF matrix to display similarity of the words (instead of the documents) in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = [LBD_02_data_preprocessing.strDomainDefault]*len(word_list)\n",
    "LBD_06_visualization.visualize_tfidf_pca_interactive(word_list, domains_list, tfidf_matrix, transpose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script filters, sorts and analyzes *rare* words within a corpus based on their maximum TF-IDF values.\n",
    "\n",
    "A word or n-gram is rare in the input documents if it only occurs in a relatively small portion of them. In this script, we assume that a term is rare if it only occurs in a single document. Note, however, that such a restriction is very sensitive to the addition of new elements to the input documents, since the rarity/frequency of a term in a text corpus can change by adding new texts to the existing input corpus. A rare term can become more frequent if the document added to the input file contains this term.\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Filtering rare words*:\n",
    "   The script first selects words that appear in only one document:\n",
    "   ```python\n",
    "   max_word_tfidf_selected = {}\n",
    "   for word in max_word_tfidf.keys():\n",
    "       if sum_count_docs_containing_word[word] <= 1:\n",
    "           max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "   ```\n",
    "   - *filtering criteria*: Words appearing in only one document are considered rare and are selected for further analysis.\n",
    "\n",
    "2. *Displaying and sorting words*:\n",
    "   The script then prints and sorts these rare words by their maximum TF-IDF value:\n",
    "   ```python\n",
    "   print('Selected rare words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "   max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "   print('Sorted rare words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "   ```\n",
    "   - *sorting*: Rare words are sorted in descending order by their TF-IDF scores, highlighting the most important terms.\n",
    "\n",
    "3. *Analyzing the results*:\n",
    "   The script calculates the mean TF-IDF value of the sorted rare words:\n",
    "   ```python\n",
    "   print('Mean value of max TF-IDF values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] <= 1:\n",
    "        max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "         \n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected rare words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted rare words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted rare word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max TF-IDF values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a few rare terms for further analysis. In our experiment [1], the autism expert identified three rare terms *calcium channel*, *synaptophysin*, and *lactoylglutathione* that appeared in our dataset, prompting the autism expert to specifically search for their similarities and associations with autism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_terms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "rare_terms_list_length = len(rare_terms_list)\n",
    "\n",
    "df = pd.DataFrame({'Rare term': rare_terms_list, 'max TF-IDF': list(max_word_tfidf_selected_sorted.values())})\n",
    "# display the first 25 rare terms\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a file\n",
    "with open('B_strings_list.txt', 'w') as file:\n",
    "    for string in rare_terms_list:\n",
    "        file.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the presence and the position of the expert selected rare terms in the candidate list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'calcium channel'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'synaptophysin'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'lactoylglutathione'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expert had searched through the list of 495 rare term cadidates. In the last part of the step Ra the expert had identified three rare terms for further exploration: \n",
    "\n",
    "* *calcium channel* (position 38/495), \n",
    "* *synaptophysin* (position 37/495), \n",
    "* and *lactoylglutathione* (position 377/495)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Jo\n",
    "\n",
    "How the input files were prepared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'input/f_calcium_channels.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'input/f_synaptophysin.txt'\n",
    "lines2 = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines2[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'input/f_lactoylglutathione.txt'\n",
    "lines3 = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines3[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all three input texts to a sinlge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.extend(lines2)\n",
    "lines.extend(lines3)\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11666 documents were collected from all three domains. The next step is to pre-process the list of input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = []\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = mesh_word_list, \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 5, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 7 dictionary items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Bag of Words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 1\n",
    "min_df = 1\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary: ', len(word_list))\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3\n",
    "keep_only_mesh_II = True\n",
    "\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        if (not keep_only_mesh_II) or (word in mesh_word_list):\n",
    "            tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            check_ngram = word.split()\n",
    "            passed = True\n",
    "            for check_word in check_ngram:\n",
    "                if keep_only_mesh_II:\n",
    "                    if check_word not in mesh_word_list:\n",
    "                        passed = False\n",
    "            if check_ngram[0] == check_ngram[1]:\n",
    "                passed = False\n",
    "            if passed:\n",
    "                tmp_filter_columns.append(i)\n",
    "\n",
    "# keep the original order of rows\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Explain the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate domains_bow_matrix from bow_matrix using domain_names list to add bow_matrix rows for each unique domain name into a single row\n",
    "domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "for i, domain_name in enumerate(unique_domains_list):\n",
    "    domain_docs_indices = [i for i, label in enumerate(domains_list) if label == domain_name]\n",
    "    print(domain_docs_indices[:7])\n",
    "    tmp = (bow_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    print(i, tmp)\n",
    "    domains_bow_matrix = np.vstack((domains_bow_matrix, tmp))\n",
    "    # Compute centroid for the current cluster\n",
    "    #centroid_x = np.mean(pca_result[cluster_docs_indices, 0])\n",
    "    #centroid_y = np.mean(pca_result[cluster_docs_indices, 1])\n",
    "print(domains_bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Explain the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_value_in_bow_matrix(bow_matrix, domain_name, word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_idx = unique_domains_list.index(domain_name)\n",
    "    column_idx = word_list.index(word)\n",
    "    return(bow_matrix[line_idx, column_idx])\n",
    "\n",
    "cell_value_in_bow_matrix(domains_bow_matrix, unique_domains_list[0], word_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print(dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print(dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print(dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print(filtered_ids_list[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary: ',len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print(dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print(dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print(dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print(dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('Unique domain names: ', unique_domains_list)\n",
    "print('Number of documents in each unique domain: ', )\n",
    "for unique_domain in unique_domains_list:\n",
    "    print('   ', unique_domain, ': ', domains_list.count(unique_domain), sep='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, domains_list, tfidf_matrix, transpose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = ['default']*len(word_list)\n",
    "LBD_06_visualization.visualize_tfidf_pca_interactive(word_list, domains_list, tfidf_matrix, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] >= 10:\n",
    "        passed = True\n",
    "        for domain_name in unique_domains_list:\n",
    "            if cell_value_in_bow_matrix(domains_bow_matrix, domain_name, word) <= 0:\n",
    "                passed = False\n",
    "        if passed:\n",
    "            max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "         \n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected common words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted joint words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted joint word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max tfidf values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_terms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "joint_terms_list_length = len(joint_terms_list)\n",
    "\n",
    "df = pd.DataFrame({'Joint term': joint_terms_list, 'max TF-IDF': list(max_word_tfidf_selected_sorted.values())})\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'calcineurin'\n",
    "print(name, ': ', 'position in the list of joint terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the step Jo we have identified a joint term for further exploration: *calcineurin*.\n",
    "So, the Literature *C* is *autism* and the Literature *A* is *calcineurin*. In step Link the tesk is to search for linking b-terms that connect the two domains *C* and *A*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step Link implements closed discovery principle between two domains, in our case `autism` and`calcineurin`. It is implemented in `LBD_mini_CrossBee.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
