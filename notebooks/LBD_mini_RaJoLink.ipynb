{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RaJoLink workflow notebook demo based on Python script modules for LBD\n",
    "\n",
    "RaJoLink is a method developed for **open** literature-based discovery (LBD). In contrast to Swanson's `ABC` model, RaJoLink focuses on a semi-automatic identification of candidates ($a$) that could be related to an investigated phenomenon ($c$). This selection is based on the identification of **rare terms** from the literature on $c$. At the heart of the RaJoLink strategy is the rational, unavoidable assumption that if literatures of multiple rare terms share a term in common, then that term becomes a candidate for term $a$.\n",
    "\n",
    "The **RaJoLink** method comprises three main steps: **Ra**, **Jo**, and **Link**, which focus on rare terms, joint terms and linking terms, respectively. The Ra step searches the literature on phenomenon $C$ for unique or rare terms. The Jo step reviews articles related to these rare terms, and identifies joint terms (candidates for $a$) that appear in them, suggesting the hypothesis that $C$ is related to $A$. The Link step then looks for $b$-terms that bridge the literature on a selected $a$-term and $c$-term; $b$-terms are the candidates that can potentially explain the link.\n",
    "\n",
    "The identification of rare terms in the Ra step is based on the statistical principle of outliers. Just as outliers in data can lead to significant discoveries, rare terms in the literature can pave the way for innovative connections. A term is considered rare if it occurs in $n$ or fewer data sets, where $n$ is adjustable depending on the experiment or context.\n",
    "\n",
    "While Swanson's ABC model connects two disjoint literatures with the term $b$, RaJoLink uses rare terms to create the term $a$, which bridges the literatures with selected rare terms.\n",
    "\n",
    "In this particular implementation, the search for b-terms is limited to the [MeSH](https://www.nlm.nih.gov/mesh/meshhome.html) words for Enzymes and Coenzymes [D08] and Amino Acids, Peptides, and Proteins [D12] selected by the domain expert. The main purpose of MeSH filtering is to reduce the vocabulary size, which in turn improves the time complexity of the LBD algorithms used. Considering only the words from the two MeSH categories also reduces the effort for the human expert to guide and evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize `logging` library for tracing the scripts execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from the framework notebooks. The purpose of the **import_ipynb** library is to enable the direct import of Jupyter Notebooks as modules, allowing code, functions, and classes defined in a notebook to be easily reused in other notebooks or python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import json\n",
    "import spacy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the script for the first time, `en_core_web_md`, which is a medium-sized English model trained on written web text (blogs, news, comments), has to be downloaded with the command:\n",
    "\n",
    "```python\n",
    "!python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "So, for the first run, you have to uncomment the first line in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md \n",
    "\n",
    "nlpr = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting file to ascii helper\n",
    "# LBD_01_data_acquisition.convert_file_to_ascii_encoding('input/214Texts.txt', 'input/214Texts_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name of the domain $C$ and load the responding text from the file. The expected file format is as follows:\n",
    "\n",
    "1. The file is encoded in ascii or utf-8 standard.\n",
    "2. Each line in the file represents one document. The words in each document are separated by spaces. Each document has a different length.\n",
    "3. The first word in each line is the **unique id**, followed by a semicolon. Normally **pmid** (pubmed id) can be used for this purpose.\n",
    "4. The second word in each line can optionally stand for a predefined domain (or class) of the document. In this case, the second word is preceded by **!**. For example, if the file contains documents that originate from two domains, e.g. *migraine* and *magnesium*, the second word in each line is either **!migraine** or **!magnesium**.\n",
    "5. If the second word is not preceded by **!**, it is considered the first word of the document. In this case, the **!NA** (**not applicable** or **not available**) domain is assigned to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainName = 'Autism'\n",
    "fileName = 'input/214Texts.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "# display the first 7 lines of the document\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove before publishing\n",
    "#domainName = 'Demo'\n",
    "#fileName = 'input/demo_2.txt'\n",
    "#lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "#lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purposes - remove later\n",
    "#lines = LBD_01_data_acquisition.load_data_from_file(\"input/Migraine_before1988.txt\")\n",
    "#lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove before publishing\n",
    "# plato's dialogues\n",
    "#lines = LBD_01_data_acquisition.load_data_from_file(\"input/plato_dialogues_ontogen.txt\")\n",
    "#lines[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess MESH words for Enzymes and Coenzymes [D08] and Amino Acids, Peptides, and Proteins [D12]. The input file MESH_D08_D12.txt after the preprocessing contains 3534 words that are used in further preprocessing of autism files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script is part of a pipeline designed to preprocess medical literature data, focusing on terms related to specific MeSH (Medical Subject Headings) categories. he purpose is to load and preprocess MESH words for Enzymes and Coenzymes [D08] and Amino Acids, Peptides, and Proteins [D12]. The input file MESH_D08_D12.txt was obtained by selecting relevant [D08] and [D12] terms from xml file `desc2024.xml` obtained from <a href=\"https://www.nlm.nih.gov/databases/download/mesh.html\">MeSH web page</a>. The input file after the preprocessing contains 3534 words that are used as filer in further preprocessing of autism-related files.\n",
    "\n",
    "*Functionality*\n",
    "\n",
    "1. **Loading Data**:  \n",
    "   The script starts by loading MeSH data from a specified file:\n",
    "   ```python\n",
    "   mesh_lines = LBD_01_data_acquisition.load_data_from_file(\"input/MESH_D08_D12.txt\")\n",
    "   ```\n",
    "   This line reads the file `MESH_D08_D12.txt`, which likely contains textual data related to specific MeSH categories (D08 and D12), into a list of lines (`mesh_lines`). The function `load_data_from_file` handles the file reading, which is a crucial step in preparing the data for analysis.\n",
    "\n",
    "2. **Dictionary Construction**:  \n",
    "   The next step constructs a dictionary from the loaded lines:\n",
    "   ```python\n",
    "   mesh_docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(mesh_lines)\n",
    "   ```\n",
    "   This transforms the list of lines into a dictionary (`mesh_docs_dict`), where keys may represent document identifiers and values hold the corresponding text. This structure is more efficient for subsequent text processing tasks.\n",
    "\n",
    "3. **Document Preprocessing**:  \n",
    "   The preprocessing phase is vital for cleaning and standardizing the textual data:\n",
    "   ```python\n",
    "   mesh_prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "       mesh_docs_dict, keep_list=[], remove_list=[], mesh_word_list=[], \\\n",
    "       cleaning=True, remove_stopwords=True, lemmatization=True, \\\n",
    "       min_word_length=5, keep_only_nouns=False, keep_only_mesh=False, stemming=False, stem_type=None)\n",
    "   ```\n",
    "   Here, various preprocessing techniques are applied:\n",
    "   - **Cleaning**: General cleaning of the text.\n",
    "   - **Stopword Removal**: Eliminating common words that do not contribute meaningful information (e.g., \"the\", \"and\").\n",
    "   - **Lemmatization**: Reducing words to their base or root form (e.g., \"running\" becomes \"run\").\n",
    "   - **Minimum Word Length**: Filtering out words shorter than five characters.\n",
    "   These steps refine the data, ensuring it is suitable for further analysis and reducing noise.\n",
    "\n",
    "4. **Extracting Preprocessed Documents**:  \n",
    "   After preprocessing, the script extracts the cleaned text back into a list:\n",
    "   ```python\n",
    "   mesh_prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(mesh_prep_docs_dict)\n",
    "   ```\n",
    "   This conversion is necessary for feature extraction, where the text needs to be in a list format.\n",
    "\n",
    "5. **Feature Extraction (Bag of Words)**:  \n",
    "   The final part of the script creates a Bag of Words (BoW) model:\n",
    "   ```python\n",
    "   mesh_word_list, mesh_bow_matrix = LBD_03_feature_extraction.create_bag_of_words(mesh_prep_docs_list, 1, 1)\n",
    "   ```\n",
    "   The BoW model is a fundamental text representation technique, where:\n",
    "   - **`mesh_word_list`** contains the vocabulary—unique words identified in the documents.\n",
    "   - **`mesh_bow_matrix`** is a matrix where each row corresponds to a document, and each column represents a word, with the matrix values indicating the frequency of words within the documents.\n",
    "\n",
    "*Practical applications*\n",
    "\n",
    "- **Biomedical Research**: Researchers can use this script to preprocess and analyze large datasets of medical literature, helping to identify new links between diseases, drugs, and biological processes.\n",
    "- **Text Mining and NLP**: The script can be adapted for broader text mining tasks, such as sentiment analysis, topic modeling, or any domain requiring structured text representation.\n",
    "\n",
    "*Usage*\n",
    "\n",
    "To use this script in your workflow, ensure you have the appropriate data file (`MESH_D08_D12.txt`) and the modules for data acquisition, preprocessing, and feature extraction. After running the script, you'll obtain a vocabulary list and a corresponding BoW matrix, ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_lines = LBD_01_data_acquisition.load_data_from_file(\"input/MESH_D08_D12.txt\")\n",
    "\n",
    "mesh_docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(mesh_lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = []\n",
    "mesh_prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    mesh_docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 5, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "mesh_prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(mesh_prep_docs_dict)\n",
    "\n",
    "mesh_word_list, mesh_bow_matrix = LBD_03_feature_extraction.create_bag_of_words(mesh_prep_docs_list, 1, 1)\n",
    "print('Number of terms in MESH D08 and D12 vocabulary: ', len(mesh_word_list))\n",
    "\n",
    "print(mesh_word_list.index('lactoylglutathione'))\n",
    "print(mesh_word_list.index('calcium'))\n",
    "print(mesh_word_list.index('channel'))\n",
    "print(mesh_word_list.index('synaptophysin'))\n",
    "\n",
    "print(mesh_word_list[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the domain documents stored in `lines` into a dictionary and then preprocess the documents using the obtained MeSH word list of Enzymes and Coenzymes [D08] and Amino Acids, Peptides, and Proteins [D12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = []\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = mesh_word_list, \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 5, keep_only_nouns = False, keep_only_mesh = True, stemming = False, stem_type = None)\n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first dictionary items, document (pubmed) ids and preprocessed documents. Then, display document `ids` and the first few preprocessed documents.\n",
    "Observe the difference between the original and preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_docs_list[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate bag-of-words matrix from the list of preprocessed documents. Remove n-gram words that occur less than *min_ngram_count* times (3 in our case) in the whole corpus of documents. Remove also the words that are not pressent in the MESH list *mesh_word_list*.\n",
    "\n",
    "This script is a continuation of the feature extraction process, focusing on refining a Bag of Words (BoW) model by filtering out less significant terms and n-grams. It generates a bag-of-words matrix from the list of preprocessed documents. Then it removes n-gram words that occur less than *min_ngram_count* times (3 in our case) in the whole corpus of documents. It removes also the words that are not pressent in the MESH list *mesh_word_list*. This step is essential for improving the quality and relevance of the text representation by reducing the vocabulary size for more efficient (time) execution of the following steps.\n",
    "\n",
    "*Functionality*\n",
    "\n",
    "1. **Setting Parameters**:  \n",
    "   The script begins by defining the parameters for n-gram size and minimum document frequency:\n",
    "   ```python\n",
    "   ngram_size = 2\n",
    "   min_df = 1\n",
    "   ```\n",
    "   - **`ngram_size`**: Specifies that the model will consider pairs of consecutive words (bigrams) as features.\n",
    "   - **`min_df`**: Sets the minimum number of documents a word or n-gram must appear in to be included in the initial vocabulary.\n",
    "\n",
    "2. **Creating Bag of Words Representation**:  \n",
    "   The next step generates the BoW model using the specified n-gram size:\n",
    "   ```python\n",
    "   word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "   print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "   ```\n",
    "   This function creates a vocabulary (`word_list`) of all terms and n-grams found in the preprocessed documents (`prep_docs_list`), along with their corresponding frequency matrix (`bow_matrix`). The initial vocabulary includes all n-grams without filtering.\n",
    "\n",
    "3. **Filtering Low-Frequency N-Grams**:  \n",
    "   The script then filters out n-grams that occur less frequently than a specified threshold:\n",
    "   ```python\n",
    "   min_count_ngram = 3\n",
    "   tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "   tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "   ```\n",
    "   - **`min_count_ngram`**: Sets the minimum occurrence count for n-grams to be retained.\n",
    "   - The script calculates two key metrics:\n",
    "     - **Document Frequency**: How many documents contain each word or n-gram.\n",
    "     - **Total Frequency**: How often each word or n-gram appears across all documents.\n",
    "\n",
    "4. **Filtering Based on Specific Criteria**:  \n",
    "   The script applies a more sophisticated filtering process to refine the vocabulary:\n",
    "   ```python\n",
    "   tmp_filter_columns = []\n",
    "   for i, word in enumerate(word_list):\n",
    "       if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "           if word in mesh_word_list:\n",
    "               tmp_filter_columns.append(i)\n",
    "       else:\n",
    "           if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "               check_ngram = word.split()\n",
    "               passed = True\n",
    "               for check_word in check_ngram:\n",
    "                   if check_word not in mesh_word_list:\n",
    "                       passed = False\n",
    "               if check_ngram[0] == check_ngram[1]:\n",
    "                   passed = False\n",
    "               if passed:\n",
    "                   tmp_filter_columns.append(i)\n",
    "   ```\n",
    "   This loop evaluates each term or n-gram in the vocabulary:\n",
    "   - **Non-n-grams**: Retained only if they are present in a predefined `mesh_word_list`.\n",
    "   - **N-Grams**: Retained if:\n",
    "     - They meet the minimum frequency criteria.\n",
    "     - All component words are in `mesh_word_list`.\n",
    "     - The n-gram does not consist of repeated words (e.g., \"word word\").\n",
    "\n",
    "5. **Applying the Filters**:  \n",
    "   The script then filters both the rows and columns of the BoW matrix:\n",
    "   ```python\n",
    "   tmp_filter_rows = []\n",
    "   for i, id in enumerate(ids_list):\n",
    "       tmp_filter_rows.append(i)\n",
    "\n",
    "   tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "       word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "   word_list = tmp_filtered_word_list\n",
    "   bow_matrix = tmp_filtered_bow_matrix\n",
    "   print('Number of terms in preprocessed vocabulary after removing infrequent n-grams and non MESH words: ', len(word_list))\n",
    "   ```\n",
    "   - **`filter_matrix_columns`**: Refines the BoW matrix by keeping only the selected words or n-grams that passed the filtering criteria.\n",
    "   - The updated vocabulary and matrix are then stored back in `word_list` and `bow_matrix`, respectively.\n",
    "\n",
    "*Applications*\n",
    "\n",
    "- **Medical Text Mining**: This filtering process is particularly useful in medical research, where the focus is on extracting and analyzing relevant biomedical terms and concepts.\n",
    "- **Document Classification**: By refining the feature set, this script can improve the performance of classifiers used in categorizing scientific literature or other text corpora.\n",
    "- **Network Analysis**: The filtered vocabulary can serve as nodes in a network graph, representing meaningful terms and their co-occurrences, which can be analyzed to identify hidden connections.\n",
    "\n",
    "*Usage*\n",
    "\n",
    "To use this script effectively, ensure you have a preprocessed document list (`prep_docs_list`) and a list of MeSH terms (`mesh_word_list`). Adjust the parameters like `ngram_size`, `min_df`, and `min_count_ngram` according to your specific needs. After running the script, you will obtain a filtered vocabulary and a corresponding BoW matrix, which are more suited for further analysis, such as clustering, topic modeling, or discovering novel hypotheses in biomedical research.\n",
    "\n",
    "In n-grams, all the words must be in mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 2\n",
    "min_df = 1\n",
    "keep_only_mesh_II = True\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        if (not keep_only_mesh_II) or (word in mesh_word_list):\n",
    "            tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            check_ngram = word.split()\n",
    "            passed = True\n",
    "            for check_word in check_ngram:\n",
    "                if keep_only_mesh_II:\n",
    "                    if check_word not in mesh_word_list:\n",
    "                        passed = False\n",
    "            if check_ngram[0] == check_ngram[1]:\n",
    "                passed = False\n",
    "            # passed = True\n",
    "            if passed:\n",
    "                tmp_filter_columns.append(i)\n",
    "\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "\n",
    "kom_text = ''\n",
    "if keep_only_mesh_II:\n",
    "    kom_text = ' and non-MeSH words'\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams', kom_text, ': ', len(word_list), sep='')\n",
    "\n",
    "LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is a continuation of the text preprocessing pipeline, computing margins for Bag of Words (BoW) matrix and optimizing the BoW matrix for better interpretability and analysis. By rearranging the matrix to emphasize the most significant terms and documents, this script aids in revealing patterns in the data, which is a crucial step in LBD.\n",
    "\n",
    "*Functionality*\n",
    "\n",
    "1. **Counting Word Frequencies**:\n",
    "   The script begins by calculating various frequency counts that provide insight into how words are distributed across documents:\n",
    "   ```python\n",
    "   sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "   sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "   sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "   ```\n",
    "   - **`sum_count_docs_containing_word`**: Counts how many documents each word appears in.\n",
    "   - **`sum_count_word_in_docs`**: Totals the occurrences of each word across all documents.\n",
    "   - **`sum_count_words_in_doc`**: Tallies the total number of words in each document.\n",
    "\n",
    "   These metrics are essential for understanding the significance and distribution of terms within the corpus, which can guide further analysis.\n",
    "\n",
    "2. **Displaying Frequency Counts**:\n",
    "   The script then prints a subset of these frequency counts to give an overview of the data:\n",
    "   ```python\n",
    "   print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "   print('Number of occurrences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "   print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "   ```\n",
    "   - **`islice`** from `itertools` is used to print just the first few items, making it easier to inspect the data without overwhelming output.\n",
    "   - These print statements help users quickly assess the distribution and frequency of words and documents in the BoW model.\n",
    "\n",
    "3. **Optimizing the BoW Matrix**:\n",
    "   The script proceeds to rearrange the BoW matrix so that the most frequent words and documents are positioned at the top-left corner of the matrix:\n",
    "   ```python\n",
    "   filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "       LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "   filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "       LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list)\n",
    "   ```\n",
    "   - **Sorting**: The words and documents are sorted by their frequencies in descending order.\n",
    "   - **Filtering**: The indices of these sorted words and documents are then used to rearrange the BoW matrix.\n",
    "\n",
    "   This step ensures that the most significant terms and documents are easily accessible, facilitating further analysis such as clustering, topic modeling, or visualization.\n",
    "\n",
    "4. **Rearranging the Matrix**:\n",
    "   Finally, the script filters the matrix according to the computed order:\n",
    "   ```python\n",
    "   filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "       ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "   ```\n",
    "   - **`filter_matrix`**: This function reorders the BoW matrix based on the sorted indices, ensuring that the most relevant terms and documents are emphasized.\n",
    "\n",
    "   The script then prints out the first few items in the reordered lists:\n",
    "   ```python\n",
    "   print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "   print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])\n",
    "   ```\n",
    "   - This output allows users to verify that the matrix has been rearranged as intended, highlighting the most important elements of the dataset.\n",
    "\n",
    "*Usage*\n",
    "\n",
    "To use this script, ensure you have a BoW matrix (`bow_matrix`) and associated lists of words (`word_list`) and document IDs (`ids_list`). The script will process these inputs to compute frequency counts, reorder the matrix, and then output the rearranged BoW matrix. This optimized matrix can be used in various downstream tasks, such as creating visualizations, conducting deeper statistical analysis, or feeding into machine learning models for predictive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize bag-of-words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tf-idf matrix from the list of preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "# Rearange (filter) the tfidf matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams and non MESH words: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print('Sum of tfidf for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of tfidf for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the tfidf matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of the domain names of all the documents and a list of unique domain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the documents in 2D graph by reducing the dimensionality of tfidf matrix with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tfidf_matrix)\n",
    "print(tfidf_matrix[:7,:7])\n",
    "print(tfidf_matrix.shape)\n",
    "print(len(ids_list))\n",
    "print(len(domains_list))\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, domains_list, tfidf_matrix, transpose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Cluster the documents using KMeans\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(tfidf_matrix)\n",
    "cluster_assignments = list(np.asarray(kmeans.labels_))\n",
    "cluster_assignments = [str(i) for i in cluster_assignments]\n",
    "type(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, cluster_assignments, tfidf_matrix, transpose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = [LBD_02_data_preprocessing.strDomainDefault]*len(word_list)\n",
    "LBD_06_visualization.visualize_tfidf_pca_interactive(word_list, domains_list, tfidf_matrix, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of *rare* words. Use the max value of word tf-idf for sorting the list of rare words.\n",
    "\n",
    "TODO: describe the criterion for rare term selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] <= 1:\n",
    "        max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "         \n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected rare words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted rare words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted rare word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max tfidf values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine three rare terms for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_terms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "rare_terms_list_length = len(rare_terms_list)\n",
    "\n",
    "df = pd.DataFrame({'Rare term': rare_terms_list, 'max tfidf': list(max_word_tfidf_selected_sorted.values())})\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_display_first = 0\n",
    "term_display_length = 400 # 75\n",
    "print('Length: ', rare_terms_list_length, ', starting with: ', term_display_first)\n",
    "for i in range(term_display_first, term_display_first+term_display_length):\n",
    "    if i < rare_terms_list_length:\n",
    "        term = rare_terms_list[i]\n",
    "        print(i, ' ', term, ' ', max_word_tfidf_selected_sorted[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'calcium channel'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'synaptophysin'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'lactoylglutathione'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the step Ra we have identified three rare terms for further exploration: \n",
    "\n",
    "* *calcium channel*, \n",
    "* *synaptophysin*, \n",
    "* and *lactoylglutathione*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Jo\n",
    "\n",
    "How the input files were prepared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'input/f_calcium_channels.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'input/f_synaptophysin.txt'\n",
    "lines2 = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines2[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'input/f_lactoylglutathione.txt'\n",
    "lines3 = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines3[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all three input texts to a sinlge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.extend(lines2)\n",
    "lines.extend(lines3)\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the list of input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = []\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = mesh_word_list, \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 5, keep_only_nouns = True, keep_only_mesh = True, stemming = False, stem_type = None)\n",
    "\n",
    "# \n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Bag of Words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 1\n",
    "min_df = 1\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(bow_matrix)\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        if word in mesh_word_list:\n",
    "            tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            check_ngram = word.split()\n",
    "            passed = True\n",
    "            for check_word in check_ngram:\n",
    "                if check_word not in mesh_word_list:\n",
    "                    passed = False\n",
    "            if check_ngram[0] == check_ngram[1]:\n",
    "                passed = False\n",
    "            # passed = True\n",
    "            if passed:\n",
    "                tmp_filter_columns.append(i)\n",
    "\n",
    "# keep the original order of rows\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary: ', len(word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate domains_bow_matrix from bow_matrix using domain_names list to add bow_matrix rows for each unique domain name into a single row\n",
    "domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "for i, domain_name in enumerate(unique_domains_list):\n",
    "    domain_docs_indices = [i for i, label in enumerate(domains_list) if label == domain_name]\n",
    "    print(domain_docs_indices[:7])\n",
    "    tmp = (bow_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    print(i, tmp)\n",
    "    domains_bow_matrix = np.vstack((domains_bow_matrix, tmp))\n",
    "    # Compute centroid for the current cluster\n",
    "    #centroid_x = np.mean(pca_result[cluster_docs_indices, 0])\n",
    "    #centroid_y = np.mean(pca_result[cluster_docs_indices, 1])\n",
    "print(domains_bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_value_in_bow_matrix(bow_matrix, domain_name, word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_idx = unique_domains_list.index(domain_name)\n",
    "    column_idx = word_list.index(word)\n",
    "    return(bow_matrix[line_idx, column_idx])\n",
    "\n",
    "cell_value_in_bow_matrix(domains_bow_matrix, unique_domains_list[0], word_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print(dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print(dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print(dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print(filtered_ids_list[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary: ',len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print(dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print(dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print(dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print(dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('Unique domain names: ', unique_domains_list)\n",
    "print('Number of documents in each unique domain: ', )\n",
    "for unique_domain in unique_domains_list:\n",
    "    print('   ', unique_domain, ': ', domains_list.count(unique_domain), sep='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, domains_list, tfidf_matrix, transpose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = ['default']*len(word_list)\n",
    "LBD_06_visualization.visualize_tfidf_pca_interactive(word_list, domains_list, tfidf_matrix, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] >= 10:\n",
    "        passed = True\n",
    "        for domain_name in unique_domains_list:\n",
    "            if cell_value_in_bow_matrix(domains_bow_matrix, domain_name, word) <= 0:\n",
    "                passed = False\n",
    "        if passed:\n",
    "            max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "         \n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected common words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted joint words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted joint word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max tfidf values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_terms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "joint_terms_list_length = len(joint_terms_list)\n",
    "\n",
    "df = pd.DataFrame({'Joint term': joint_terms_list, 'max tfidf': list(max_word_tfidf_selected_sorted.values())})\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'calcineurin'\n",
    "print(name, ': ', 'position in the list of joint terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'kappa'\n",
    "print(list(max_word_tfidf_selected_sorted.keys()))\n",
    "# print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "#      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the step Jo we have identified a joint term for further exploration: *calcineurin*.\n",
    "So, the Literature *C* is *autism* and the Literature *A* is *calcineurin*. In step Link the tesk is to search for linking b-terms that connect the two domains *C* and *A*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"apple banana fruit\",\n",
    "    \"apple orange juice\",\n",
    "    \"banana orange juice\",\n",
    "    \"apple apple orange\",\n",
    "    \"banana banana fruit\"\n",
    "]\n",
    "\n",
    "# Create a BoW matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert BoW matrix to numpy array\n",
    "bow_matrix = X.toarray()\n",
    "\n",
    "# Compute in how many documents each word appears\n",
    "word_counts = (bow_matrix > 0).sum(axis=0)\n",
    "\n",
    "# Display the results\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for word, count in zip(words, word_counts):\n",
    "    print(f\"The word '{word}' appears in {count} document(s).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary of 20 words with random integer values\n",
    "words = [\"word\" + str(i) for i in range(20)]\n",
    "values = [random.randint(1, 10) for _ in range(20)]  # random integers between 1 and 10\n",
    "dictionary = dict(zip(words, values))\n",
    "\n",
    "# Compute the frequency distribution of the integer numbers\n",
    "frequency_distribution = defaultdict(int)\n",
    "for value in dictionary.values():\n",
    "    frequency_distribution[value] += 1\n",
    "\n",
    "# Sort the frequency distribution by the integer numbers (keys) in ascending order\n",
    "sorted_distribution = dict(sorted(frequency_distribution.items()))\n",
    "\n",
    "# Display the dictionary and the frequency distribution\n",
    "print(\"Dictionary:\")\n",
    "for word, value in dictionary.items():\n",
    "    print(f\"{word}: {value}\")\n",
    "\n",
    "print(\"\\nFrequency Distribution:\")\n",
    "for number, count in sorted_distribution.items():\n",
    "    print(f\"Number {number} appears {count} time(s).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"pear banana fruit\",\n",
    "    \"pear orange juice\",\n",
    "    \"banana orange juice\",\n",
    "    \"apple apple orange\",\n",
    "    \"banana banana fruit\"\n",
    "]\n",
    "\n",
    "# Create a BoW matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert BoW matrix to numpy array\n",
    "bow_matrix = X.toarray()\n",
    "\n",
    "# Compute in how many documents each word appears\n",
    "word_document_counts = (bow_matrix > 0).sum(axis=0)\n",
    "\n",
    "# Display the results for words appearing in multiple documents\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for word, doc_count in zip(words, word_document_counts):\n",
    "    print(f\"The word '{word}' appears in {doc_count} document(s).\")\n",
    "\n",
    "# For words that appear in only one document, compute how many times they appear\n",
    "single_document_words = np.where(word_document_counts == 1)[0]\n",
    "for idx in single_document_words:\n",
    "    word = words[idx]\n",
    "    frequency = bow_matrix[:, idx].max()\n",
    "    print(f\"\\nThe word '{word}' appears in only one document, {frequency} time(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove - irrelevant\n",
    "import nltk\n",
    "word_pos_tag_list = []\n",
    "for word in word_list:\n",
    "    word_pos_tag_list.append(nltk.pos_tag([word]))\n",
    "word_pos_tag_list[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nltk.pos_tag([name])\n",
    "x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlpr(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlpr(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlpr(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlpr(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlpr(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(text):\n",
    "    \"\"\"Return POS tags for individual terms in the given text.\"\"\"\n",
    "    doc = nlpr(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Example\n",
    "text = \"calcineurin calcium channels lactoyglutathione The quick brown fox jumps over the lazy dog.\"\n",
    "tags = get_pos_tags(text)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"Generate n-grams from tokens.\"\"\"\n",
    "    return [tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def get_pos_tags_for_ngrams(text, n=2):\n",
    "    \"\"\"Return POS tags for individual n-grams in the given text.\"\"\"\n",
    "    doc = nlpr(text)\n",
    "    \n",
    "    # Convert Doc into a list of tokens\n",
    "    tokens = [token for token in doc]\n",
    "    \n",
    "    # Generate n-grams from the list of tokens\n",
    "    ngrams = generate_ngrams(tokens, n)\n",
    "    \n",
    "    # Return n-grams and their POS tags\n",
    "    return [(\" \".join([token.text for token in ngram]), [token.pos_ for token in ngram]) for ngram in ngrams]\n",
    "\n",
    "# Example\n",
    "text = \"cr256abc aa b df mrs. 00000 0234g 111234 red green calcium channel The quick brown fox jumps over the lazy dog Apple bojan Nada Andrej.\"\n",
    "tags = get_pos_tags(text)\n",
    "bigrams = get_pos_tags_for_ngrams(text, 2)\n",
    "trigrams = get_pos_tags_for_ngrams(text, 3)\n",
    "\n",
    "print(tags)\n",
    "print(\"Unigrams:\")\n",
    "for uni in tags:\n",
    "    print(uni)\n",
    "    # print(uni[0], ' ', uni[1])\n",
    "\n",
    "print(\"Bigrams:\")\n",
    "for bi in bigrams:\n",
    "    print(bi)\n",
    "\n",
    "print(\"\\nTrigrams:\")\n",
    "for tri in trigrams:\n",
    "    print(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
