{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossBee workflow demo based on Python Script Modules for LBD Text Mining Analysis\n",
    "\n",
    "CrossBee is a system that recommends bridging terms (*b-terms*) through an ensemble-based ranking method. It aids experts in uncovering hidden connections between unrelated domains. The user-friendly CrossBee web application facilitates the ranking, exploration, and efficient investigation of these cross-domain links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LBD_01_data_acquisition.ipynb\n",
      "importing Jupyter notebook from LBD_02_data_preprocessing.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LBD_03_feature_extraction.ipynb\n",
      "importing Jupyter notebook from LBD_04_text_mining.ipynb\n",
      "importing Jupyter notebook from LBD_05_results_analysis.ipynb\n",
      "importing Jupyter notebook from LBD_06_visualization.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import json\n",
    "import spacy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define domain name and load text from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 10:15:29: INFO - Loaded 16156 lines from \"input/Magnesium_Migraine_before1988.txt\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1Mag: !Mag Characterization of Clostridium thermocellum JW20.\\n',\n",
       " '2Mag: !Mag Organochlorine and heavy metal contamination in the eggs of the Spanish Imperial Eagle (Aquila (heliaca) adalberti) and accompanying changes in eggshell morphology and chemistry.\\n',\n",
       " '3Mag: !Mag A molecular orbital study of a model of the Mg2+ coordination complex of the self splicing reaction of ribosomal RNA.\\n',\n",
       " '4Mag: !Mag Distribution and diagenesis of microfossils from the lower Proterozoic Duck Creek Dolomite, Western Australia.\\n',\n",
       " '5Mag: !Mag Microfossils from oolites and pisolites of the Upper Proterozoic Eleonore Bay Group, Central East Greenland.\\n',\n",
       " '6Mag: !Mag Carbonates and sulfates in CI chondrites:  formation by aqueous activity on the parent body.\\n',\n",
       " '7Mag: !Mag The influence of prebiotic-type organic molecules on the crystallization of Al and Mg hydroxides.\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainName = 'Migraine-Magnesium'\n",
    "fileName = 'input/Magnesium_Migraine_before1988.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess documents into a dictionary and extract documents as strings in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 10:15:41: INFO - Text cleaning ...\n",
      "2024-08-11 10:15:41: INFO - Removing stopwords ...\n",
      "2024-08-11 10:15:41: INFO - Lematization ...\n",
      "2024-08-11 10:15:44: INFO - Preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = [\"p\"]\n",
    "remove_list = [\"migraine\", \"magnesium\"]\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 0, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first dictionary items, document (pubmed) ids and preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1Mag': {'domain': 'Mag',\n",
       "  'document': 'Characterization of Clostridium thermocellum JW20.',\n",
       "  'preprocessed': 'characterization clostridium thermocellum jw20'},\n",
       " '2Mag': {'domain': 'Mag',\n",
       "  'document': 'Organochlorine and heavy metal contamination in the eggs of the Spanish Imperial Eagle (Aquila (heliaca) adalberti) and accompanying changes in eggshell morphology and chemistry.',\n",
       "  'preprocessed': 'organochlorine heavy metal contamination egg spanish imperial eagle aquila heliaca adalberti accompany change eggshell morphology chemistry'},\n",
       " '3Mag': {'domain': 'Mag',\n",
       "  'document': 'A molecular orbital study of a model of the Mg2+ coordination complex of the self splicing reaction of ribosomal RNA.',\n",
       "  'preprocessed': 'molecular orbital study model mg2 coordination complex self splice reaction ribosomal rna'},\n",
       " '4Mag': {'domain': 'Mag',\n",
       "  'document': 'Distribution and diagenesis of microfossils from the lower Proterozoic Duck Creek Dolomite, Western Australia.',\n",
       "  'preprocessed': 'distribution diagenesis microfossils lower proterozoic duck creek dolomite western australia'},\n",
       " '5Mag': {'domain': 'Mag',\n",
       "  'document': 'Microfossils from oolites and pisolites of the Upper Proterozoic Eleonore Bay Group, Central East Greenland.',\n",
       "  'preprocessed': 'microfossils oolites pisolites upper proterozoic eleonore bay group central east greenland'},\n",
       " '6Mag': {'domain': 'Mag',\n",
       "  'document': 'Carbonates and sulfates in CI chondrites: formation by aqueous activity on the parent body.',\n",
       "  'preprocessed': 'carbonate sulfate ci chondrites formation aqueous activity parent body'},\n",
       " '7Mag': {'domain': 'Mag',\n",
       "  'document': 'The influence of prebiotic-type organic molecules on the crystallization of Al and Mg hydroxides.',\n",
       "  'preprocessed': 'influence prebiotictype organic molecules crystallization al mg hydroxides'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['characterization clostridium thermocellum jw20',\n",
       " 'organochlorine heavy metal contamination egg spanish imperial eagle aquila heliaca adalberti accompany change eggshell morphology chemistry',\n",
       " 'molecular orbital study model mg2 coordination complex self splice reaction ribosomal rna',\n",
       " 'distribution diagenesis microfossils lower proterozoic duck creek dolomite western australia',\n",
       " 'microfossils oolites pisolites upper proterozoic eleonore bay group central east greenland',\n",
       " 'carbonate sulfate ci chondrites formation aqueous activity parent body',\n",
       " 'influence prebiotictype organic molecules crystallization al mg hydroxides']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_docs_list[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate bag-of-words matrix from the list of preprocessed documents. Remove ngram words that occur less than *min_ngram_count* times (3 in our case) in the whole corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in initial vocabulary with all n-grams:  162022\n",
      "Number of terms in preprocessed vocabulary after removing infrequent n-grams:  29841\n"
     ]
    }
   ],
   "source": [
    "ngram_size = 3\n",
    "min_df = 1\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(bow_matrix)\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 2\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        # if word in mesh_word_list:\n",
    "        tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            tmp_filter_columns.append(i)\n",
    "\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams: ', len(word_list))\n",
    "\n",
    "LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for bag-of-word matrix. Prepare also the dictionaries of words and documents sorted according to the sum of word count in BOW matrix (*bow_matrix*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in which each word is present:  {'000dalton': 1, '1030nm': 1, '10formyltetrahydrofolate': 1, '10year': 1, '10yearold': 1, '110kd': 2, '110orthophenanthroline': 1}\n",
      "Number of occurences of each word in all documents:  {'000dalton': 1, '1030nm': 1, '10formyltetrahydrofolate': 1, '10year': 1, '10yearold': 1, '110kd': 2, '110orthophenanthroline': 1}\n",
      "Number of words in each document:  {'1Mag': 4, '2Mag': 17, '3Mag': 15, '4Mag': 11, '5Mag': 15, '6Mag': 9, '7Mag': 9}\n",
      "The first few documents in the rows of the filtered bow matrix:  ['9710Mag', '9711Mag', '1632Mag', '6221Mag', '7063Mag', '7064Mag', '586Mag']\n",
      "The first few words in the columns of the filtered bow matrix:  ['effect', 'calcium', 'study', 'rat', 'headache', 'treatment', 'patients']\n"
     ]
    }
   ],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize bag-of-words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tf-idf matrix from the list of preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all ngrams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "# Rearange (filter) the tfidf matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent ngrams: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print('Sum of tfidf for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of tfidf for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the tfidf matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of the domain names of all the documents and a list of unique domain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate domains_bow_matrix from bow_matrix using domain_names list to add bow_matrix rows for each unique domain name into a single row\n",
    "domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "no_documents_in_domain = {}\n",
    "for i, domain_name in enumerate(unique_domains_list):\n",
    "    domain_docs_indices = [i for i, label in enumerate(domains_list) if label == domain_name]\n",
    "    no_documents_in_domain[domain_name] = len(domain_docs_indices)\n",
    "    print(domain_docs_indices[:7])\n",
    "    tmp = (bow_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    print(i, tmp)\n",
    "    domains_bow_matrix = np.vstack((domains_bow_matrix, tmp))\n",
    "    # Compute centroid for the current cluster\n",
    "    #centroid_x = np.mean(pca_result[cluster_docs_indices, 0])\n",
    "    #centroid_y = np.mean(pca_result[cluster_docs_indices, 1])\n",
    "print(domains_bow_matrix)\n",
    "print(no_documents_in_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_value_in_bow_matrix(bow_matrix, domain_name, word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_idx = unique_domains_list.index(domain_name)\n",
    "    column_idx = word_list.index(word)\n",
    "    return(bow_matrix[line_idx, column_idx])\n",
    "\n",
    "cell_value_in_bow_matrix(domains_bow_matrix, unique_domains_list[0], word_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "sum_word_tfidf_selected = {}\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] >= 1:\n",
    "        passed = True\n",
    "        for domain_name in unique_domains_list:\n",
    "            if cell_value_in_bow_matrix(domains_bow_matrix, domain_name, word) <= 0:\n",
    "                passed = False\n",
    "        if passed:\n",
    "            max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "            sum_word_tfidf_selected[word] = sum_word_tfidf[word]\n",
    "         \n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected bterm candidate words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "sum_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(sum_word_tfidf_selected, True)\n",
    "\n",
    "print()\n",
    "\n",
    "print('MAX Sorted bterm candidate words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted bterm word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max tfidf values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())\n",
    "\n",
    "print()\n",
    "\n",
    "print('SUM Sorted bterm candidate words: ', len(sum_word_tfidf_selected_sorted), ' ', dict(itertools.islice(sum_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted bterm word: ', list(sum_word_tfidf_selected_sorted.items())[0], ' ', list(sum_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of sum tfidf values: ', np.array(list(sum_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bterms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "bterms_list = list(sum_word_tfidf_selected_sorted.keys())\n",
    "bterms_list_length = len(bterms_list)\n",
    "\n",
    "# df = pd.DataFrame({'b-term': bterms_list, 'max tfidf': list(max_word_tfidf_selected_sorted.values())})\n",
    "df = pd.DataFrame({'b-term': bterms_list, 'sum tfidf': list(sum_word_tfidf_selected_sorted.values())})\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'calcium blockers'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')\n",
    "print(name, ': ', 'position in the list of rare terms ', list(sum_word_tfidf_selected_sorted.keys()).index(name), ' (', len(sum_word_tfidf_selected_sorted), \\\n",
    "      '), sum tfidf: ', format(sum_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX\n",
    "swansons_bterms = [\"5ht\", \"5hydroxytryptamine\", \"5hydroxytryptamine receptors\", \"aggregation\", \"inflammatory\", \\\n",
    "                   \"anticonvulsant\", \"antimigraine\", \"arterial spasms\", \"brain serotonin\", \"calcium antagonist\", \"calcium blockers\", \\\n",
    "                   \"calcium channel\", \"calcium channel blockers\", \"cerebral vasospasm\", \"convulsions\", \"convulsive\", \"coronary spasm\", \\\n",
    "                   \"cortical spread depression\", \"diltiazem\", \"epilepsy\", \"epileptic\", \"epileptiform\", \"hypoxia\", \"indomethacin\", \\\n",
    "                   \"inflammatory\", \"nifedipine\", \"paroxysmal\", \"platelet aggregation\", \"platelet function\", \"prostacyclin\", \\\n",
    "                   \"prostaglandin\", \"prostaglandin e1\", \"prostaglandin synthesis\", \"reactivity\", \"seizure\", \"serotonin\", \\\n",
    "                   \"spasm\", \"spread\", \"spread depression\", \"stress\", \"substancep\", \"vasospasm\", \"verapamil\"]\n",
    "\n",
    "nn = 0\n",
    "indb = []\n",
    "size = len(max_word_tfidf_selected_sorted)\n",
    "for name in swansons_bterms:\n",
    "    if name in max_word_tfidf_selected_sorted.keys():\n",
    "       nn += 1\n",
    "       position = list(max_word_tfidf_selected_sorted.keys()).index(name)\n",
    "       indb.append(position)\n",
    "       print(name, ': ', 'position in the list of joint terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "             '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), ' part: ', format(position/size*100, '.1f'), sep='')\n",
    "    else:\n",
    "        print('NOT:', name, 'NOT in the list.')   \n",
    "print(nn, len(swansons_bterms))\n",
    "print(indb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUM\n",
    "swansons_bterms = [\"5ht\", \"5hydroxytryptamine\", \"5hydroxytryptamine receptors\", \"aggregation\", \"inflammatory\", \\\n",
    "                   \"anticonvulsant\", \"antimigraine\", \"arterial spasms\", \"brain serotonin\", \"calcium antagonist\", \"calcium blockers\", \\\n",
    "                   \"calcium channel\", \"calcium channel blockers\", \"cerebral vasospasm\", \"convulsions\", \"convulsive\", \"coronary spasm\", \\\n",
    "                   \"cortical spread depression\", \"diltiazem\", \"epilepsy\", \"epileptic\", \"epileptiform\", \"hypoxia\", \"indomethacin\", \\\n",
    "                   \"inflammatory\", \"nifedipine\", \"paroxysmal\", \"platelet aggregation\", \"platelet function\", \"prostacyclin\", \\\n",
    "                   \"prostaglandin\", \"prostaglandin e1\", \"prostaglandin synthesis\", \"reactivity\", \"seizure\", \"serotonin\", \\\n",
    "                   \"spasm\", \"spread\", \"spread depression\", \"stress\", \"substancep\", \"vasospasm\", \"verapamil\"]\n",
    "\n",
    "nn = 0\n",
    "indb = []\n",
    "size = len(sum_word_tfidf_selected_sorted)\n",
    "for name in swansons_bterms:\n",
    "    if name in sum_word_tfidf_selected_sorted.keys():\n",
    "       nn += 1\n",
    "       position = list(sum_word_tfidf_selected_sorted.keys()).index(name)\n",
    "       indb.append(position)\n",
    "       print(name, ': ', 'position in the list of joint terms ', list(sum_word_tfidf_selected_sorted.keys()).index(name), ' (', len(sum_word_tfidf_selected_sorted), \\\n",
    "             '), sum tfidf: ', format(sum_word_tfidf_selected_sorted[name], '.3f'), ' part: ', format(position/size*100, '.1f'), sep='')\n",
    "    else:\n",
    "        print('NOT:', name, 'NOT in the list.')   \n",
    "print(nn, len(swansons_bterms))\n",
    "print(indb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing b-terms:\n",
    "\n",
    "- 5hydroxytryptamine receptors - found only in Migraine !Mig documents\n",
    "- antimigraine - found only in Migraine !Mig documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement selected heuristics for bterm ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = []\n",
    "for i in range(size):\n",
    "    pt.append(0)\n",
    "for i in range(len(indb)):\n",
    "    for j in range(indb[i], size):\n",
    "        pt[j] += 1\n",
    "print(pt)\n",
    "suma = 0\n",
    "part = 0\n",
    "suma_array = []\n",
    "part_array = []\n",
    "for i in range(size):\n",
    "    print((i+1)/size*100.0, pt[i]/len(indb)*100.0)\n",
    "    part += pt[i]\n",
    "    suma += len(indb)\n",
    "    part_array.append((i+1)/size)\n",
    "    suma_array.append(pt[i]/len(indb))\n",
    "print(part/suma*100.0)\n",
    "\n",
    "no_all_bterm_candidates = size\n",
    "no_swansons_bterms = len(indb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(part_array)\n",
    "print(suma_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example ROC curve points (y-values), replace with the actual list of 120 numbers\n",
    "roc_points = pt\n",
    "\n",
    "# X-values for the ROC curve (0 to 120)\n",
    "x_values = np.arange(0, no_all_bterm_candidates)\n",
    "\n",
    "# Calculating the AUC for the given ROC curve using the trapezoidal rule\n",
    "auc = np.trapz(roc_points, x_values) / (no_all_bterm_candidates*no_swansons_bterms) * 100  # Normalizing by the area of the full plot\n",
    "\n",
    "# Plotting the default curve (50% AUC)\n",
    "default_x = np.array([0, no_all_bterm_candidates])\n",
    "default_y = np.array([0, no_swansons_bterms])\n",
    "plt.plot(default_x, default_y, label='Default Curve (50% AUC)', linestyle='--', color='gray')\n",
    "\n",
    "# Plotting the given ROC curve\n",
    "plt.plot(x_values, roc_points, label=f'Given ROC Curve (AUC: {auc:.2f})', color='blue')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('Number of candidates for b-terms (odered with a selected heuristics)')\n",
    "plt.ylabel('Number of actual b-terms found')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Synthetic Data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train Multiple Classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Neural Network\": MLPClassifier(max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute ROC Curves and AUC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "    fpr = part_array\n",
    "    tpr = suma_array\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "\n",
    "# Step 4: Plot ROC Curves\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
