{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction Module\n",
    "\n",
    "This module implements various feature extraction techniques like Bag of Words, TF-IDF, and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function create_bag_of_words takes a list of preprocessed text documents as input and uses the scikit-learn library \n",
    "to create a bag of words representation of the corpus. \n",
    "The function first creates an instance of the sklearn.feature_extraction.text.CountVectorizer, \n",
    "which is an implementation of the bag of words model.\n",
    "\n",
    "The function then fits the vectorizer to the input corpus and transforms the corpus into a bag of words matrix using \n",
    "the vectorizer.fit_transform() method. The matrix is stored in the Compressed Sparse Row (CSR) format (csr_matrix) \n",
    "for efficient storage and computation.\n",
    "\n",
    "Finally, the function returns a tuple containing the CountVectorizer instance and the bag of words matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(corpus: List[str], ngram_size=1, min_df=1) -> Tuple[List, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create a bag of words representation of a text corpus.\n",
    "    :param corpus: List[str], a list of preprocessed text documents\n",
    "    :ngram_size: max ngram size, default = 1 meaning single words\n",
    "    :min_df: ignore words that have a document frequency strictly lower than the given threshold, default = 1\n",
    "             if the value is between 0 and 0.999, it ignores the words with relative document frequency strictly lower than the threshold\n",
    "     :return: Tuple[List, Matrix], the List of words and the bow matrix\n",
    "    \"\"\"\n",
    "    # Create an instance of the CountVectorizer from scikit-learn\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, ngram_size), min_df=min_df)\n",
    "    # vectorizer = CountVectorizer(ngram_range=(1, ngram_size), min_df=min_df, stop_words='english')\n",
    "\n",
    "    # Fit the vectorizer to the corpus and transform the corpus into a bag of words matrix\n",
    "    bag_of_words_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Return the vectorizer and the bag of words matrix as a tuple\n",
    "    return vectorizer.get_feature_names_out().tolist(), bag_of_words_matrix.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for summarizing bag_of_words and tfidf matices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_is_nterm(word: str):\n",
    "    return ' ' in word\n",
    "\n",
    "def sum_count_documents_containing_each_word(word_list: List, bow_matrix: np.ndarray) -> Dict[str, int]:\n",
    "    # number of documents containing each word\n",
    "    word_counts = (bow_matrix > 0).sum(axis=0)\n",
    "    words_dict = {}\n",
    "    for word, count in zip(word_list, word_counts):\n",
    "        words_dict[word] = count\n",
    "    return words_dict\n",
    "\n",
    "def sum_count_each_word_in_all_documents(word_list: List, any_matrix: np.ndarray) -> Dict[str, int]:\n",
    "    # number of occurences of each word in all documents; sum of matrix columns\n",
    "    word_counts = (any_matrix).sum(axis=0)\n",
    "    if type(word_counts) == np.matrix:\n",
    "        word_counts = word_counts.tolist()[0]\n",
    "    words_dict = {}\n",
    "    for word, count in zip(word_list, word_counts):\n",
    "        words_dict[word] = count\n",
    "    return words_dict\n",
    "\n",
    "def max_tfidf_each_word_in_all_documents(word_list: List, tfidf_matrix: np.ndarray) -> Dict[str, int]:\n",
    "    # max tfidf of each word in all documents; max element found in each matrix column\n",
    "    word_counts = (tfidf_matrix).max(axis=0)\n",
    "    if type(word_counts) == np.matrix:\n",
    "        word_counts = word_counts.tolist()[0]\n",
    "    words_dict = {}\n",
    "    for word, count in zip(word_list, word_counts):\n",
    "        words_dict[word] = count\n",
    "    return words_dict\n",
    "\n",
    "def sum_count_all_words_in_each_document(ids_list: List, any_matrix: np.ndarray) -> Dict[str, int]:\n",
    "    # number of words in each document; sum of matrix rows\n",
    "    word_counts = (any_matrix).sum(axis=1)\n",
    "    if type(word_counts) == np.matrix:\n",
    "        word_counts = word_counts.transpose().tolist()[0]\n",
    "    words_dict = {}\n",
    "    for id, count in zip(ids_list, word_counts):\n",
    "        words_dict[id] = count\n",
    "    return words_dict\n",
    "\n",
    "def max_tfidf_all_words_in_each_document(ids_list: List, tfidf_matrix: np.ndarray) -> Dict[str, int]:\n",
    "    # max tfidf of words in each document; max element found in each matrix row\n",
    "    word_counts = (tfidf_matrix).max(axis=1)\n",
    "    if type(word_counts) == np.matrix:\n",
    "        word_counts = word_counts.transpose().tolist()[0]\n",
    "    words_dict = {}\n",
    "    for id, count in zip(ids_list, word_counts):\n",
    "        words_dict[id] = count\n",
    "    return words_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for extracting sub-matrix from a given matrix (bag_of_words and tfidf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matrix(ids_list: List, word_list: List, any_matrix: np.ndarray, filter_rows: List, filter_columns: List) -> Tuple[List, List, np.ndarray]:\n",
    "    # filter the input matrix (according to filter_rows and filter_columns) into output matrix and preserve the order of filters in new output matrix \n",
    "    \"\"\"\n",
    "    Construct a sub-matrix from the given matrix based on filter_rows and filter_columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - matrix (numpy.ndarray): The original matrix.\n",
    "    - filter_rows (list): List of row indices to include in the sub-matrix.\n",
    "    - filter_columns (list): List of column indices to include in the sub-matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The sub-matrix.\n",
    "    \"\"\"\n",
    "    new_ids_list = []\n",
    "    for ind in filter_rows:\n",
    "        new_ids_list.append(ids_list[ind])\n",
    "    new_word_list = []\n",
    "    for ind in filter_columns:\n",
    "        new_word_list.append(word_list[ind])\n",
    "    return new_ids_list, new_word_list, any_matrix[np.ix_(filter_rows, filter_columns)]\n",
    "\n",
    "def filter_matrix_columns(word_list: List, any_matrix: np.ndarray, filter_rows: List, filter_columns: List) -> Tuple[List, np.ndarray]:\n",
    "    # filter columns of the input matrix (according to filter_rows and filter_columns) into output matrix and preserve the order of filters in new output matrix \n",
    "    \"\"\"\n",
    "    Construct a sub-matrix from the given matrix based on filter_columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - matrix (numpy.ndarray): The original matrix.\n",
    "    - filter_rows (list): List of row indices to include in the sub-matrix.\n",
    "    - filter_columns (list): List of column indices to include in the sub-matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The sub-matrix.\n",
    "    \"\"\"\n",
    "    new_word_list = []\n",
    "    for ind in filter_columns:\n",
    "        new_word_list.append(word_list[ind])\n",
    "    return new_word_list, any_matrix[np.ix_(filter_rows, filter_columns)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function create_tfidf takes a list of preprocessed text documents as input and uses the scikit-learn library \n",
    "to create a Term Frequency-Inverse Document Frequency (TF-IDF) representation of the corpus. \n",
    "The function first creates an instance of the sklearn.feature_extraction.text.TfidfVectorizer, \n",
    "which is an implementation of the TF-IDF model.\n",
    "\n",
    "The function then fits the vectorizer to the input corpus and transforms the corpus into a TF-IDF matrix using \n",
    "the vectorizer.fit_transform() method. The matrix is stored in the Compressed Sparse Row (CSR) format (csr_matrix) \n",
    "for efficient storage and computation.\n",
    "\n",
    "Finally, the function returns a tuple containing the list of features *TfidfVectorizer.get_feature_names_out().tolist()* that correspond to the columns of the TF-IDF matrix and the TF-IDF matrix *tfidf_matrix.todense()*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(corpus: List[str], ngram_size=1, min_df=1) -> Tuple[List, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create a TF-IDF representation of a text corpus.\n",
    "    :param corpus: List[str], a list of preprocessed text documents\n",
    "    :ngram_size: max ngram size, default = 1 meaning single words\n",
    "    :min_df: ignore words that have a document frequency strictly lower than the given threshold, default = 1\n",
    "             if the value is between 0 and 0.999, it ignores the words with relative document frequency strictly lower than the threshold\n",
    "    :return: TTuple[List, Matrix], the List of words and the tf-idf matrix\n",
    "    \"\"\"\n",
    "    # Create an instance of the TfidfVectorizer from scikit-learn\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, ngram_size), min_df=min_df)\n",
    "    # vectorizer = TfidfVectorizer(ngram_range=(1, ngram_size), min_df=min_df, stop_words='english')\n",
    "\n",
    "    # Fit the vectorizer to the corpus and transform the corpus into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Return the vectorizer and the TF-IDF matrix as a tuple\n",
    "    return vectorizer.get_feature_names_out().tolist(), tfidf_matrix.todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_matrix(ids_list: List, word_list: List, any_matrix: np.ndarray, cluster_rows: List) -> np.ndarray:\n",
    "    # cluster the input matrix (according to cluster_rows) into output matrix, where the number of rows is the number of clusters \n",
    "    \"\"\"\n",
    "    Construct an agregate sub-matrix from the given matrix based on cluster_rows.\n",
    "    cluster_rows contains integer numbers representing index of a cluster of the corresponding document.\n",
    "    The clusters are represented by numbers 0, 1, 2, ... \n",
    "    \n",
    "    Parameters:\n",
    "    - matrix (numpy.ndarray): The original matrix.\n",
    "    - filter_rows (list): List of row indices to include in the sub-matrix.\n",
    "    - filter_columns (list): List of column indices to include in the sub-matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The sub-matrix with the row dimensionality of number of different clusters.\n",
    "    \"\"\"\n",
    "    new_ids_list = []\n",
    "    for ind in cluster_rows:\n",
    "        new_ids_list.append(ids_list[ind])\n",
    "    return any_matrix[np.ix_(cluster_rows, cluster_rows)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function create_word_embeddings takes a list of tokenized text documents and an optional embedding method \n",
    "(either 'word2vec' or 'fasttext', defaulting to 'word2vec') as input, and uses the gensim library to create \n",
    "word embeddings for the documents. The function first trains a word embedding model using the specified method, \n",
    "creating either a Word2Vec or FastText model with a vector size of 100, a window size of 5, a minimum word count of 1, \n",
    "and 4 worker threads for parallelization.\n",
    "\n",
    "The function then initializes an empty matrix with the shape (len(tokens_list), model.vector_size) to store the document embeddings. \n",
    "For each document in the input tokens_list, the function calculates the document embedding by averaging the word embeddings \n",
    "of each token in the document. This is done by first retrieving the word embeddings for each token using the trained \n",
    "model's model.wv[token] attribute, then calculating the mean of these embeddings along axis 0.\n",
    "\n",
    "Finally, the function returns a tuple containing the word embedding model (either a Word2Vec or FastText instance) \n",
    "and the document embeddings matrix as a NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_embeddings(tokens_list: List[List[str]], embedding_method: str = 'word2vec') -> Tuple[BaseEstimator, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create word embeddings for a list of tokenized documents using the specified embedding method.\n",
    "    :param tokens_list: List[List[str]], a list of tokenized text documents\n",
    "    :param embedding_method: str, the embedding method to use, either 'word2vec' or 'fasttext' (default: 'word2vec')\n",
    "    :return: Tuple[BaseEstimator, np.ndarray], the word embedding model and the document embeddings matrix\n",
    "    \"\"\"\n",
    "    # Train the word embedding model based on the specified method\n",
    "    if embedding_method == 'word2vec':\n",
    "        model = Word2Vec(tokens_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    elif embedding_method == 'fasttext':\n",
    "        model = FastText(tokens_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding method. Please use either 'word2vec' or 'fasttext'.\")\n",
    "\n",
    "    # Initialize an empty matrix to store the document embeddings\n",
    "    document_embeddings = np.zeros((len(tokens_list), model.vector_size))\n",
    "\n",
    "    # Calculate the document embeddings by averaging the word embeddings of each token\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        token_embeddings = np.array([model.wv[token] for token in tokens])\n",
    "        document_embeddings[i] = token_embeddings.mean(axis=0)\n",
    "\n",
    "    return model, document_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function create_bag_of_words takes a list of preprocessed text documents as input and creates a bag of words representation of the corpus without using the scikit-learn or scipy packages. The function first defines a helper function tokenize_and_count that takes a text document as input, tokenizes it by splitting on whitespace, and counts the occurrences of each token using a defaultdict.\n",
    "\n",
    "The function then initializes an empty dictionary word_to_index to map words to their index, and an empty list bag_of_words_list to store the bag of words representation of each document. For each document in the input corpus, the function tokenizes and counts the occurrences of words using the tokenize_and_count helper function, and appends the resulting dictionary to the bag_of_words_list.\n",
    "\n",
    "For each unique word in the word count dictionary, the function checks if it is already in the word_to_index dictionary. If not, the word is added to the dictionary with a new index, which is the current length of the dictionary.\n",
    "\n",
    "Finally, the function returns a tuple containing the word_to_index dictionary and the bag_of_words_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words_deprecated(corpus: List[str]) -> Tuple[Dict[str, int], List[Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Create a bag of words representation of a text corpus without using scikit-learn or scipy.\n",
    "    :param corpus: List[str], a list of preprocessed text documents\n",
    "    :return: Tuple[Dict[str, int], List[Dict[str, int]]], a dictionary mapping words to their index, \n",
    "             and a list of dictionaries representing the bag of words for each document\n",
    "    \"\"\"\n",
    "    def tokenize_and_count(document: str) -> Dict[str, int]:\n",
    "        tokens = document.split()\n",
    "        word_count = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            word_count[token] += 1\n",
    "        return word_count\n",
    "\n",
    "    word_to_index = {}\n",
    "    bag_of_words_list = []\n",
    "\n",
    "    for document in corpus:\n",
    "        word_count = tokenize_and_count(document)\n",
    "        bag_of_words_list.append(word_count)\n",
    "\n",
    "        for word in word_count:\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "\n",
    "    return word_to_index, bag_of_words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
