{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossBee workflow demo based on Python Script Modules for LBD Text Mining Analysis\n",
    "\n",
    "CrossBee is a system that recommends bridging terms (*b-terms*) through an ensemble-based ranking method. It aids experts in uncovering hidden connections between unrelated domains. The user-friendly CrossBee web application facilitates the ranking, exploration, and efficient investigation of these cross-domain links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize `logging` library to track the execution of the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import json\n",
    "import spacy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define domain name and load text from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainName = 'Migraine-Magnesium'\n",
    "fileName = 'input/Magnesium_Migraine_before1988.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainName = 'Autism-Calcineurin'\n",
    "fileName = 'input/Autism_Calcineurin.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainName = 'Alzheimer-Macrobiota'\n",
    "fileName = 'input/61_Alz_GIMB_Crossbee.txt'\n",
    "fileName = 'input/f_alzheimer_gimb_outliers_1.txt'\n",
    "fileName = 'input/f_alzheimer_gimb_outliers_2.txt'\n",
    "fileName = 'input/f_alzheimer_gimb_outliers_3.txt'\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "lines[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#domainName = 'Alzheimer-Macrobiota'\n",
    "#fileName = 'input/6_Alz_GIMB_Ontogen.txt'\n",
    "#lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "#lines[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess documents into a dictionary and extract documents as strings in a list. Might take a few minutes for longer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "keep_list = []\n",
    "remove_list = ['alzheimer', 'macrobiota']\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 4, keep_only_nouns = False)\n",
    "\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first dictionary items, document (pubmed) ids and preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(itertools.islice(prep_docs_dict.items(), 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_docs_list[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate bag-of-words matrix from the list of preprocessed documents. Remove ngram words that occur less than *min_ngram_count* times (3 in our case) in the whole corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size = 3\n",
    "min_df = 1\n",
    "\n",
    "# BOW representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all ngrams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(bow_matrix)\n",
    "\n",
    "# remove nterms with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3 # 2 for migraine\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        # if word in mesh_word_list:\n",
    "        tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            tmp_filter_columns.append(i)\n",
    "\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent ngrams: ', len(word_list))\n",
    "\n",
    "LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for bag-of-word matrix. Prepare also the dictionaries of words and documents sorted according to the sum of word count in BOW matrix (*bow_matrix*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize bag-of-words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered bag of words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tf-idf matrix from the list of preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all ngrams: ', len(word_list))\n",
    "# print(word_list)\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "# Rearange (filter) the tfidf matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent ngrams: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute margins for tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "print('Sum of tfidf for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of tfidf for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of tfidf for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# Compute the order of rows (documents) and columns (words) in the tfidf matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "# Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of the domain names of all the documents and a list of unique domain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate domains_bow_matrix from bow_matrix using domain_names list to add bow_matrix rows for each unique domain name into a single row\n",
    "domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "no_documents_in_domain = {}\n",
    "for i, domain_name in enumerate(unique_domains_list):\n",
    "    domain_docs_indices = [i for i, label in enumerate(domains_list) if label == domain_name]\n",
    "    no_documents_in_domain[domain_name] = len(domain_docs_indices)\n",
    "    print(domain_docs_indices[:7])\n",
    "    tmp = (bow_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    print(i, tmp)\n",
    "    domains_bow_matrix = np.vstack((domains_bow_matrix, tmp))\n",
    "    # Compute centroid for the current cluster\n",
    "    #centroid_x = np.mean(pca_result[cluster_docs_indices, 0])\n",
    "    #centroid_y = np.mean(pca_result[cluster_docs_indices, 1])\n",
    "print(domains_bow_matrix)\n",
    "print(no_documents_in_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_value_in_bow_matrix(bow_matrix, domain_name, word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_idx = unique_domains_list.index(domain_name)\n",
    "    column_idx = word_list.index(word)\n",
    "    return(bow_matrix[line_idx, column_idx])\n",
    "\n",
    "cell_value_in_bow_matrix(domains_bow_matrix, unique_domains_list[0], word_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary of words, count and max(tfidf):\")\n",
    "\n",
    "max_word_tfidf_selected = {}\n",
    "sum_word_tfidf_selected = {} ### TODO from her on ...\n",
    "for word in max_word_tfidf.keys():\n",
    "    if sum_count_docs_containing_word[word] >= 1:\n",
    "        passed = True\n",
    "        for domain_name in unique_domains_list:\n",
    "            if cell_value_in_bow_matrix(domains_bow_matrix, domain_name, word) <= 0:\n",
    "                passed = False\n",
    "        if passed:\n",
    "            max_word_tfidf_selected[word] = max_word_tfidf[word]\n",
    "         \n",
    "import itertools\n",
    "print('All the words in vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected bterm candidate words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted bterm candidate words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('First and last sorted bterm word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max tfidf values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bterms_list = list(max_word_tfidf_selected_sorted.keys())\n",
    "bterms_list_length = len(bterms_list)\n",
    "\n",
    "df = pd.DataFrame({'b-term': bterms_list, 'max TF-IDF': list(max_word_tfidf_selected_sorted.values())})\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1201:1225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_word_tfidf_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'nitric'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'nitric oxide'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'nitric oxide synthase'\n",
    "print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "      '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petric_bterms = [\"22q112\", \"deletion syndrome\", \"asbestos\", \"bcl2\", \"bombesin\", \"calmodulin\", \"radiation\", \\\n",
    "                 \"hypothyroxinemia\", \"synaptic\", \"synaptic plasticity\", \"type 1 diabetes\", \\\n",
    "                 \"ulcerative colitis\", \"working memory\", \\\n",
    "                 \"t17p22q21\"]\n",
    "\n",
    "petric_bterms = [\"22q112\", \"deletion\", \"syndrome\", \"asbestos\", \"bcl2\", \"bombesin\", \"calmodulin\", \"radiation\", \\\n",
    "                 \"hypothyroxinemia\", \"synaptic\", \"plasticity\", \"diabetes\", \\\n",
    "                 \"colitis\", \"work\", \"memory\", \"t17p22q21\"]\n",
    "nn = 0\n",
    "indb = []\n",
    "size = len(max_word_tfidf_selected_sorted)\n",
    "for name in petric_bterms:\n",
    "    if name in max_word_tfidf_selected_sorted.keys():\n",
    "       nn += 1\n",
    "       position = list(max_word_tfidf_selected_sorted.keys()).index(name)\n",
    "       indb.append(position)\n",
    "       print(name, ': ', 'position in the list of potential bterms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "             '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), ' part: ', format(position/size*100, '.1f'), sep='')\n",
    "    else:\n",
    "        print('NOT:', name, 'NOT in the list.')   \n",
    "print(nn, len(petric_bterms))\n",
    "print(indb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing and changed b-terms:\n",
    "\n",
    "- maternal hypothyroxinemia -> hypothyroxinemia\n",
    "- ulcerative colitis -> colitis\n",
    "- t17p22q21 - found only in Autism documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: implement selected heuristics for bterm ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = []\n",
    "for i in range(size):\n",
    "    pt.append(0)\n",
    "for i in range(len(indb)):\n",
    "    for j in range(indb[i], size):\n",
    "        pt[j] += 1\n",
    "print(pt)\n",
    "suma = 0\n",
    "part = 0\n",
    "for i in range(size):\n",
    "    print((i+1)/size*100.0, pt[i]/len(indb)*100.0)\n",
    "    part += pt[i]\n",
    "    suma += len(indb)\n",
    "print(part/suma*100.0)\n",
    "\n",
    "no_all_bterm_candidates = size\n",
    "no_swansons_bterms = len(indb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example ROC curve points (y-values), replace with the actual list of 120 numbers\n",
    "roc_points = pt\n",
    "\n",
    "# X-values for the ROC curve (0 to 120)\n",
    "x_values = np.arange(0, no_all_bterm_candidates)\n",
    "\n",
    "# Calculating the AUC for the given ROC curve using the trapezoidal rule\n",
    "auc = np.trapz(roc_points, x_values) / (no_all_bterm_candidates*no_swansons_bterms) * 100  # Normalizing by the area of the full plot\n",
    "\n",
    "# Plotting the default curve (50% AUC)\n",
    "default_x = np.array([0, no_all_bterm_candidates])\n",
    "default_y = np.array([0, no_swansons_bterms])\n",
    "plt.plot(default_x, default_y, label='Default Curve (50% AUC)', linestyle='--', color='gray')\n",
    "\n",
    "# Plotting the given ROC curve\n",
    "plt.plot(x_values, roc_points, label=f'Given ROC Curve (AUC: {auc:.2f})', color='blue')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate (Discrete)')\n",
    "plt.ylabel('True Positive Rate (Discrete)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
